{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UvcV03CNIFqO",
        "outputId": "d5a76866-7477-4ad0-a4e2-5b1ca4ab1d94"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "xGcrSqHzi1_u"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "data_train = pd.read_csv(\"/content/drive/MyDrive/liar_dataset/train.tsv\", sep=\"\\t\", header=None)\n",
        "data_valid = pd.read_csv(\"/content/drive/MyDrive/liar_dataset/valid.tsv\", sep=\"\\t\", header=None)\n",
        "data_test = pd.read_csv(\"/content/drive/MyDrive/liar_dataset/test.tsv\", sep=\"\\t\", header=None)\n",
        "\n",
        "data_train[1] = data_train[1].map({\"pants-fire\" : 5, \"barely-true\" : 4, \"half-true\" : 3, \"mostly-true\" : 2, \"true\" : 1, \"false\" : 0})\n",
        "data_valid[1] = data_valid[1].map({\"pants-fire\" : 5, \"barely-true\" : 4, \"half-true\" : 3, \"mostly-true\" : 2, \"true\" : 1, \"false\" : 0})\n",
        "data_test[1] = data_test[1].map({\"pants-fire\" : 5, \"barely-true\" : 4, \"half-true\" : 3, \"mostly-true\" : 2, \"true\" : 1, \"false\" : 0})"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Viewing sample train data before preprocessing\n",
        "data_test.head(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "Z9IafFVBjNEk",
        "outputId": "007f3c39-1216-4ebf-c778-7f13ca328fca"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "           0   1                                                  2   \\\n",
              "0  11972.json   1  Building a wall on the U.S.-Mexico border will...   \n",
              "1  11685.json   0  Wisconsin is on pace to double the number of l...   \n",
              "2  11096.json   0  Says John McCain has done nothing to help the ...   \n",
              "3   5209.json   3  Suzanne Bonamici supports a plan that will cut...   \n",
              "4   9524.json   5  When asked by a reporter whether hes at the ce...   \n",
              "\n",
              "                                                  3   \\\n",
              "0                                        immigration   \n",
              "1                                               jobs   \n",
              "2                    military,veterans,voting-record   \n",
              "3  medicare,message-machine-2012,campaign-adverti...   \n",
              "4  campaign-finance,legal-issues,campaign-adverti...   \n",
              "\n",
              "                                 4                     5          6   \\\n",
              "0                        rick-perry              Governor      Texas   \n",
              "1                 katrina-shankland  State representative  Wisconsin   \n",
              "2                      donald-trump       President-Elect   New York   \n",
              "3                     rob-cornilles            consultant     Oregon   \n",
              "4  state-democratic-party-wisconsin                   NaN  Wisconsin   \n",
              "\n",
              "           7   8    9   10  11  12                            13  \n",
              "0  republican  30   30  42  23  18               Radio interview  \n",
              "1    democrat   2    1   0   0   0             a news conference  \n",
              "2  republican  63  114  51  37  61  comments on ABC's This Week.  \n",
              "3  republican   1    1   3   1   1                  a radio show  \n",
              "4    democrat   5    7   2   2   7                   a web video  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-343a2287-9a25-47e1-b64a-c94b7a3da124\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>11972.json</td>\n",
              "      <td>1</td>\n",
              "      <td>Building a wall on the U.S.-Mexico border will...</td>\n",
              "      <td>immigration</td>\n",
              "      <td>rick-perry</td>\n",
              "      <td>Governor</td>\n",
              "      <td>Texas</td>\n",
              "      <td>republican</td>\n",
              "      <td>30</td>\n",
              "      <td>30</td>\n",
              "      <td>42</td>\n",
              "      <td>23</td>\n",
              "      <td>18</td>\n",
              "      <td>Radio interview</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>11685.json</td>\n",
              "      <td>0</td>\n",
              "      <td>Wisconsin is on pace to double the number of l...</td>\n",
              "      <td>jobs</td>\n",
              "      <td>katrina-shankland</td>\n",
              "      <td>State representative</td>\n",
              "      <td>Wisconsin</td>\n",
              "      <td>democrat</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>a news conference</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>11096.json</td>\n",
              "      <td>0</td>\n",
              "      <td>Says John McCain has done nothing to help the ...</td>\n",
              "      <td>military,veterans,voting-record</td>\n",
              "      <td>donald-trump</td>\n",
              "      <td>President-Elect</td>\n",
              "      <td>New York</td>\n",
              "      <td>republican</td>\n",
              "      <td>63</td>\n",
              "      <td>114</td>\n",
              "      <td>51</td>\n",
              "      <td>37</td>\n",
              "      <td>61</td>\n",
              "      <td>comments on ABC's This Week.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>5209.json</td>\n",
              "      <td>3</td>\n",
              "      <td>Suzanne Bonamici supports a plan that will cut...</td>\n",
              "      <td>medicare,message-machine-2012,campaign-adverti...</td>\n",
              "      <td>rob-cornilles</td>\n",
              "      <td>consultant</td>\n",
              "      <td>Oregon</td>\n",
              "      <td>republican</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>a radio show</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>9524.json</td>\n",
              "      <td>5</td>\n",
              "      <td>When asked by a reporter whether hes at the ce...</td>\n",
              "      <td>campaign-finance,legal-issues,campaign-adverti...</td>\n",
              "      <td>state-democratic-party-wisconsin</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Wisconsin</td>\n",
              "      <td>democrat</td>\n",
              "      <td>5</td>\n",
              "      <td>7</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>7</td>\n",
              "      <td>a web video</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-343a2287-9a25-47e1-b64a-c94b7a3da124')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-343a2287-9a25-47e1-b64a-c94b7a3da124 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-343a2287-9a25-47e1-b64a-c94b7a3da124');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Below function performs all the required data cleaning and preprocessing steps\n",
        "\n",
        "def data_preprocessing(dataset):\n",
        "  \n",
        "  #Dropping unwanted columns(id, counts) \n",
        "  dataset = dataset.drop(labels=[0,8,9,10,11,12] ,axis=1)\n",
        "\n",
        "  # renaming the DataFrame columns\n",
        "  data_train.rename(columns = { 1 :'label'},\n",
        "\t\t\tinplace = True)\n",
        "  data_valid.rename(columns = { 1 :'label'},\n",
        "\t\t\tinplace = True)\n",
        "  data_test.rename(columns = { 1 :'label'},\n",
        "\t\t\tinplace = True)\n",
        "\n",
        "  dataset[\"sentence\"] = dataset[2] # statement column\n",
        "  dataset = dataset.drop([2,3,4,5,6,7,13], axis=1) #dropping metadata columns, as we have merged them into a single column\n",
        "  dataset.dropna() #Dropping if there are still any null values\n",
        "\n",
        "  return dataset"
      ],
      "metadata": {
        "id": "HbfEF8ugM88_"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Applying pre-processing to the raw data - train, valid and test sets\n",
        "data_train = data_preprocessing(data_train)\n",
        "data_valid = data_preprocessing(data_valid)\n",
        "data_test = data_preprocessing(data_test)"
      ],
      "metadata": {
        "id": "va9_LJmtkCCk"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Sample data after preprocessing\n",
        "data_train.head(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "lfkbYhc4kEOE",
        "outputId": "237d9419-aaf0-4514-d584-4d5ae0e86791"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   label                                           sentence\n",
              "0      0  Says the Annies List political group supports ...\n",
              "1      3  When did the decline of coal start? It started...\n",
              "2      2  Hillary Clinton agrees with John McCain \"by vo...\n",
              "3      0  Health care reform legislation is likely to ma...\n",
              "4      3  The economic turnaround started at the end of ..."
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-ea584ca9-adba-4a3e-be31-62399ac0f82b\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "      <th>sentence</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>Says the Annies List political group supports ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>3</td>\n",
              "      <td>When did the decline of coal start? It started...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>Hillary Clinton agrees with John McCain \"by vo...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>Health care reform legislation is likely to ma...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>3</td>\n",
              "      <td>The economic turnaround started at the end of ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ea584ca9-adba-4a3e-be31-62399ac0f82b')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-ea584ca9-adba-4a3e-be31-62399ac0f82b button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-ea584ca9-adba-4a3e-be31-62399ac0f82b');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Analyzing length of sentences in training data to decide on MAX_LENGTH variable, which is required for BERT and RoBERTa\n",
        "\n",
        "sent_len = []\n",
        "for sent in data_train['sentence']:\n",
        "  sent_len.append(len(sent))\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig = plt.figure(figsize =(10, 7))\n",
        "plt.boxplot(sent_len)\n",
        "plt.show()\n",
        "\n",
        "sent_len = [i for i in sent_len if i<=500] #Excluding the outliers\n",
        "fig2 = plt.figure(figsize =(10, 7))\n",
        "plt.hist(sent_len, 5)\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "wrWuazPqkGo-",
        "outputId": "4b1dff1c-921b-49ed-e468-cd9a8bb960ad"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x700 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x700 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G7Fh-375kLAH",
        "outputId": "b2ba792f-b548-4cf9-e6c4-3dafea1fb456"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.9/dist-packages (4.27.4)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (0.13.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from transformers) (2.27.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers) (3.10.7)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (23.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (0.13.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (1.26.15)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (3.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#importing required packages  \n",
        "from transformers import (\n",
        "    BertForSequenceClassification,    \n",
        "    BertTokenizer,\n",
        "    RobertaForSequenceClassification,\n",
        "    RobertaTokenizer,\n",
        "    AdamW)"
      ],
      "metadata": {
        "id": "8F4W_FxakNUg"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading BERT base model\n",
        "bert_model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", #Using BERT base model with an uncased vocab.\n",
        "                                                                num_labels = 6, #number of output labels - 0,1,2,3,4,5 (multiclass classification)\n",
        "                                                                output_attentions = False, #model doesnt return attention weights\n",
        "                                                                output_hidden_states = False #model doesnt return hidden states\n",
        "                                                          )\n",
        "#BERT tokenizer\n",
        "bert_tokenizer = BertTokenizer.from_pretrained(\"bert-large-uncased\", do_lower_case=True)\n",
        "\n",
        "# Tell pytorch to run this model on the GPU.\n",
        "bert_model.cuda()\n",
        "# Loading RoBERTa base model\n",
        "roberta_model = RobertaForSequenceClassification.from_pretrained(\"roberta-large\", #RoBERTa base model\n",
        "                                                                    num_labels = 6,  #number of output labels - 0,1,2,3,4,5 (multiclass classification)\n",
        "                                                                    output_attentions = False,  #model doesnt return attention weights\n",
        "                                                                    output_hidden_states = False #model doesnt return hidden states\n",
        "                                                                )\n",
        "#RoBERTa tokenizer\n",
        "roberta_tokenizer = RobertaTokenizer.from_pretrained(\"roberta-large\", do_lower_case=True)\n",
        "\n",
        "# Tell pytorch to run this model on the GPU.\n",
        "roberta_model.cuda()\n",
        "\n",
        "print(' models loaded')\n"
      ],
      "metadata": {
        "id": "t2B3d-v4kQdz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8d2a1190-b57f-40b6-c0d0-cde76a006758"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of the model checkpoint at roberta-large were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'roberta.pooler.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'lm_head.dense.weight']\n",
            "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.out_proj.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " models loaded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the original sentence.\n",
        "print(' Original: ', data_train[\"sentence\"][0])\n",
        "\n",
        "# Split the sentence into tokens - BERT\n",
        "print('Tokenized BERT: ', bert_tokenizer.tokenize(data_train[\"sentence\"][0]))\n",
        "\n",
        "# Mapping tokens to token IDs - BERT\n",
        "print('Token IDs BERT: ', bert_tokenizer.convert_tokens_to_ids(bert_tokenizer.tokenize(data_train[\"sentence\"][0])))\n",
        "\n",
        "# Split the sentence into tokens -RoBERTa\n",
        "print('Tokenized RoBERT: ', roberta_tokenizer.tokenize(data_train[\"sentence\"][0]))\n",
        "\n",
        "# Mapping tokens to token IDs - RoBERTa\n",
        "print('Token IDs RoBERTa: ', roberta_tokenizer.convert_tokens_to_ids(roberta_tokenizer.tokenize(data_train[\"sentence\"][0])))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "moONYsW4m7m7",
        "outputId": "223333f4-a426-4c3e-b23c-7fba57611bc7"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Original:  Says the Annies List political group supports third-trimester abortions on demand.\n",
            "Tokenized BERT:  ['says', 'the', 'annie', '##s', 'list', 'political', 'group', 'supports', 'third', '-', 'trim', '##ester', 'abortion', '##s', 'on', 'demand', '.']\n",
            "Token IDs BERT:  [2758, 1996, 8194, 2015, 2862, 2576, 2177, 6753, 2353, 1011, 12241, 20367, 11324, 2015, 2006, 5157, 1012]\n",
            "Tokenized RoBERT:  ['S', 'ays', 'the', 'Ann', 'ies', 'List', 'political', 'group', 'supports', 'third', '-', 'tr', 'imester', 'abortions', 'on', 'demand', '.']\n",
            "Token IDs RoBERTa:  [104, 4113, 5, 3921, 918, 9527, 559, 333, 4548, 371, 12, 4328, 38417, 17600, 15, 1077, 4]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#assigning sentences and labels to separate variables\n",
        "sentences = data_train[\"sentence\"].values\n",
        "labels = data_train[\"label\"].values"
      ],
      "metadata": {
        "id": "5T7lCXgCod9Y"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch"
      ],
      "metadata": {
        "id": "rsCztvqWogc1"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Below function performs tokenization process as required by bert and roberta models, for a given dataset\n",
        "def bert_robert_tokenization(dataset):\n",
        "  sentences = dataset[\"sentence\"].values\n",
        "  labels = dataset[\"label\"].values\n",
        "  max_length = 256\n",
        "\n",
        "  # Tokenize all of the sentences and map the tokens to thier word IDs.\n",
        "  bert_input_ids = []\n",
        "  bert_attention_masks = []\n",
        "  roberta_input_ids = []\n",
        "  roberta_attention_masks = []\n",
        "\n",
        "  sentence_ids = []\n",
        "  counter = 0\n",
        "\n",
        "  # For every sentence...\n",
        "  for sent in sentences:\n",
        "      #encode_plus function will encode the sentences as required by model, including tokenization process and mapping token ids\n",
        "      bert_encoded_dict = bert_tokenizer.encode_plus(\n",
        "                          str(sent),        #sentence              \n",
        "                          add_special_tokens = True, # Add '[CLS]' and '[SEP]' tokens \n",
        "                          max_length = 256,     #Since we have seen from our analysis that majority of sentences have length less than 300.    \n",
        "                          pad_to_max_length = True,    # Pad sentences to 256 length  if the length of sentence is less than max_length\n",
        "                          return_attention_mask = True,   # Create attention mask\n",
        "                          truncation = True,  # truncate sentences to 256 length  if the length of sentence is greater than max_length\n",
        "                          return_tensors = 'pt',     # Return pytorch tensors.\n",
        "                    )\n",
        "      \n",
        "      roberta_encoded_dict = roberta_tokenizer.encode_plus(\n",
        "                          str(sent),        #sentence\n",
        "                          add_special_tokens = True, # Add '[CLS]' and '[SEP]' tokens \n",
        "                          max_length = 256,        #Since we have seen from our analysis that majority of sentences have length less than 300.   \n",
        "                          pad_to_max_length = True,     # Pad sentences to 256 length  if the length of sentence is less than max_length\n",
        "                          return_attention_mask = True,   # Create attention mask\n",
        "                          truncation = True,   # truncate sentences to 256 length  if the length of sentence is greater than max_length\n",
        "                          return_tensors = 'pt',     # Return pytorch tensors.\n",
        "                    )\n",
        "      \n",
        "    \n",
        "      # Add the encoded sentence to the list.    \n",
        "      bert_input_ids.append(bert_encoded_dict['input_ids'])\n",
        "      roberta_input_ids.append(roberta_encoded_dict['input_ids'])\n",
        "      \n",
        "      \n",
        "      # Add attention mask to the list \n",
        "      bert_attention_masks.append(bert_encoded_dict['attention_mask'])\n",
        "      roberta_attention_masks.append(roberta_encoded_dict['attention_mask'])\n",
        "      \n",
        "      \n",
        "      # collecting sentence_ids\n",
        "      sentence_ids.append(counter)\n",
        "      counter  = counter + 1\n",
        "      \n",
        "      \n",
        "      \n",
        "  # Convert the lists into tensors.\n",
        "  bert_input_ids = torch.cat(bert_input_ids, dim=0)\n",
        "  bert_attention_masks = torch.cat(bert_attention_masks, dim=0)\n",
        "\n",
        "  roberta_input_ids = torch.cat(roberta_input_ids, dim=0)\n",
        "  roberta_attention_masks = torch.cat(roberta_attention_masks, dim=0)\n",
        "\n",
        "\n",
        "  labels = torch.tensor(labels)\n",
        "  sentence_ids = torch.tensor(sentence_ids)\n",
        "\n",
        "  return {\"Bert\":[bert_input_ids, bert_attention_masks, labels], \"Roberta\":[roberta_input_ids, roberta_attention_masks, labels]}"
      ],
      "metadata": {
        "id": "R5Nt4kBnoi8P"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import TensorDataset, random_split\n",
        "# function to seed the script globally\n",
        "torch.manual_seed(0)\n",
        "\n",
        "#tokenizing train set\n",
        "token_dict_train = bert_robert_tokenization(data_train)\n",
        "\n",
        "bert_input_ids,bert_attention_masks,labels = token_dict_train[\"Bert\"]\n",
        "roberta_input_ids, roberta_attention_masks, labels = token_dict_train[\"Roberta\"]\n",
        "\n",
        "#tokenizing validation set\n",
        "token_dict_valid = bert_robert_tokenization(data_valid)\n",
        "\n",
        "bert_input_ids_valid,bert_attention_masks_valid,labels_valid = token_dict_valid[\"Bert\"]\n",
        "roberta_input_ids_valid, roberta_attention_masks_valid, labels_valid = token_dict_valid[\"Roberta\"]\n",
        "\n",
        "#tokenizing test set\n",
        "token_dict_test = bert_robert_tokenization(data_test)\n",
        "\n",
        "bert_input_ids_test,bert_attention_masks_test,labels_test = token_dict_test[\"Bert\"]\n",
        "roberta_input_ids_test, roberta_attention_masks_test, labels_test = token_dict_test[\"Roberta\"]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EjbaxTSZoxn-",
        "outputId": "f7eba0f6-1414-4918-d7de-512cb3e6b297"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/transformers/tokenization_utils_base.py:2346: FutureWarning:\n",
            "\n",
            "The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Combine the training inputs into a TensorDataset.\n",
        "bert_train_dataset = TensorDataset( bert_input_ids, bert_attention_masks, labels) \n",
        "roberta_train_dataset = TensorDataset(roberta_input_ids, roberta_attention_masks, labels)\n",
        "\n",
        "# Combine the validation inputs into a TensorDataset.\n",
        "bert_val_dataset = TensorDataset(bert_input_ids_valid,bert_attention_masks_valid,labels_valid)\n",
        "roberta_val_dataset = TensorDataset(roberta_input_ids_valid, roberta_attention_masks_valid, labels_valid)"
      ],
      "metadata": {
        "id": "sSIUxkKho0Uu"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Combine the test inputs into a TensorDataset.\n",
        "bert_test_dataset = TensorDataset(bert_input_ids_test,bert_attention_masks_test,labels_test)\n",
        "roberta_test_dataset = TensorDataset(roberta_input_ids_test, roberta_attention_masks_test, labels_test)"
      ],
      "metadata": {
        "id": "20kRQhgUo2gn"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "batch_size = 8\n",
        "\n",
        "# Create the DataLoaders for our training - Loads the data randomly in batches of size 32\n",
        "bert_train_dataloader = DataLoader(\n",
        "            bert_train_dataset,  # The training samples.\n",
        "            sampler = RandomSampler(bert_train_dataset), # Select batches randomly\n",
        "            batch_size = batch_size # Trains with this batch size.\n",
        "        )\n",
        "\n",
        "roberta_train_dataloader = DataLoader(\n",
        "            roberta_train_dataset,  # The training samples.\n",
        "            sampler = RandomSampler(roberta_train_dataset), # Select batches randomly\n",
        "            batch_size = batch_size # Trains with this batch size.\n",
        "        )\n",
        "\n",
        "# Create the DataLoaders for our validation - Loads the data in batches of size 32\n",
        "bert_validation_dataloader = DataLoader(\n",
        "            bert_val_dataset, # The validation samples.\n",
        "            sampler = SequentialSampler(bert_val_dataset), # Pull out batches sequentially.\n",
        "            batch_size = batch_size # Evaluate with this batch size.\n",
        "        )\n",
        "\n",
        "\n",
        "roberta_validation_dataloader = DataLoader(\n",
        "            roberta_val_dataset, # The validation samples.\n",
        "            sampler = SequentialSampler(roberta_val_dataset), # Pull out batches sequentially.\n",
        "            batch_size = batch_size # Evaluate with this batch size.\n",
        "        )"
      ],
      "metadata": {
        "id": "1-p2qCoYo4Ln"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# optimizers - AdamW\n",
        "# here, i have used default learning rate and epsilon values for both BERT and RoBERTa\n",
        "bert_optimizer = AdamW(bert_model.parameters(),\n",
        "                  lr = 5e-5, \n",
        "                  eps = 1e-8 \n",
        "                )\n",
        "\n",
        "roberta_optimizer = AdamW(roberta_model.parameters(),\n",
        "                  lr = 5e-5, \n",
        "                  eps = 1e-8 \n",
        "                )"
      ],
      "metadata": {
        "id": "XdnQArnWo7bu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7b74961f-24bd-4026-f2c6-92fc83f2b316"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/transformers/optimization.py:391: FutureWarning:\n",
            "\n",
            "This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import get_linear_schedule_with_warmup\n",
        "\n",
        "# Number of training epochs\n",
        "epochs = 2\n",
        "\n",
        "# Total number of training steps is [number of batches] x [number of epochs]\n",
        "total_steps = len(bert_train_dataloader) * epochs\n",
        "\n",
        "# Create the learning rate scheduler.\n",
        "bert_scheduler = get_linear_schedule_with_warmup(bert_optimizer, \n",
        "                                            num_warmup_steps = 0, \n",
        "                                            num_training_steps = total_steps)\n",
        "\n",
        "roberta_scheduler = get_linear_schedule_with_warmup(roberta_optimizer, \n",
        "                                            num_warmup_steps = 0,\n",
        "                                            num_training_steps = total_steps)"
      ],
      "metadata": {
        "id": "pD8V0tljo9Pq"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Function to calculate the accuracy of our predictions vs labels\n",
        "def flat_accuracy(preds, labels):\n",
        "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
      ],
      "metadata": {
        "id": "jACUAjvto_bO"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import datetime\n",
        "\n",
        "def format_time(elapsed):\n",
        "    '''\n",
        "    Takes a time in seconds and returns a string hh:mm:ss\n",
        "    '''\n",
        "    # Round to the nearest second.\n",
        "    elapsed_rounded = int(round((elapsed)))\n",
        "    \n",
        "    # Format as hh:mm:ss\n",
        "    return str(datetime.timedelta(seconds=elapsed_rounded))"
      ],
      "metadata": {
        "id": "Hxt2MgR4pBPc"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# tell pytorch to use the gpu if available\n",
        "if torch.cuda.is_available():    \n",
        "      \n",
        "    device = torch.device(\"cuda\")\n",
        "\n",
        "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
        "\n",
        "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
        "\n",
        "# If not...\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h8z5CIJopDc-",
        "outputId": "61c39043-5ee4-4790-aa17-f4c1725b7e26"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are 1 GPU(s) available.\n",
            "We will use the GPU: Tesla T4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os;"
      ],
      "metadata": {
        "id": "BLxNS1KRrI0R"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n"
      ],
      "metadata": {
        "id": "CtFDCx3UrGaV"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "# This training code is based on the `run_glue.py` script here:\n",
        "# https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L128\n",
        "# Set the seed value all over the place to make this reproducible.\n",
        "seed_val = 42\n",
        "random.seed(seed_val)\n",
        "np.random.seed(seed_val)\n",
        "torch.manual_seed(seed_val)\n",
        "torch.cuda.manual_seed_all(seed_val)\n",
        "loss_values = []\n",
        "for epoch_i in range(0, epochs):\n",
        "    #Training \n",
        "    print(\"\")\n",
        "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
        "    print('Training...')\n",
        "    # Measure how long the training epoch takes.\n",
        "    t0 = time.time()\n",
        "    # Reset the total loss for this epoch.\n",
        "    total_loss = 0\n",
        "    bert_model.train()\n",
        "    # For each batch of training data...\n",
        "    for step, batch in enumerate(bert_train_dataloader):\n",
        "      #Report progress after every 40 epochs\n",
        "        if step % 40 == 0 and not step == 0: \n",
        "            # Calculate elapsed time in minutes.\n",
        "            elapsed = format_time(time.time() - t0)\n",
        "            \n",
        "            # print current training batch and elapsed time\n",
        "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(bert_train_dataloader), elapsed))\n",
        "        \n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_input_mask = batch[1].to(device)\n",
        "        b_labels = batch[2].to(device)\n",
        "        \n",
        "        bert_model.zero_grad()        \n",
        "        \n",
        "        outputs = bert_model(b_input_ids, \n",
        "                    token_type_ids=None, \n",
        "                    attention_mask=b_input_mask, \n",
        "                    labels=b_labels)\n",
        "        \n",
        "        # model returns a tuple, extract loss value from that tuple\n",
        "        loss = outputs[0]\n",
        "        total_loss += loss.item()\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(bert_model.parameters(), 1.0)\n",
        "        bert_optimizer.step()\n",
        "        \n",
        "        bert_scheduler.step()\n",
        "    # Calculate the average loss over the training data.\n",
        "    avg_train_loss = total_loss / len(bert_train_dataloader)            \n",
        "    \n",
        "    # Store the loss value for plotting the learning curve.\n",
        "    loss_values.append(avg_train_loss)\n",
        "    print(\"\")\n",
        "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
        "    print(\"  Training epcoh took: {:}\".format(format_time(time.time() - t0)))\n",
        "        \n",
        "    #Validation Part\n",
        "\n",
        "    \n",
        "    print(\"\")\n",
        "    print(\"Running Validation...\")\n",
        "    t0 = time.time()\n",
        "    # Put the model in evaluation mode    \n",
        "    bert_model.eval()\n",
        "    # Tracking variables \n",
        "    eval_loss, eval_accuracy = 0, 0\n",
        "    nb_eval_steps, nb_eval_examples = 0, 0\n",
        "    # Evaluate data for one epoch\n",
        "    for batch in bert_validation_dataloader:\n",
        "        \n",
        "        # Add batch to GPU\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        \n",
        "        # Unpack the inputs from our dataloader\n",
        "        b_input_ids, b_input_mask, b_labels = batch\n",
        "        \n",
        "        with torch.no_grad():        \n",
        "           outputs = bert_model(b_input_ids, \n",
        "                            token_type_ids=None, \n",
        "                            attention_mask=b_input_mask)\n",
        "        \n",
        "        logits = outputs[0]\n",
        "        # Move logits and labels to CPU\n",
        "        logits = logits.detach().cpu().numpy()\n",
        "        label_ids = b_labels.to('cpu').numpy()\n",
        "        \n",
        "        # Calculate the accuracy for this batch of test sentences.\n",
        "        tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
        "        \n",
        "        # Accumulate the total accuracy.\n",
        "        eval_accuracy += tmp_eval_accuracy\n",
        "        # Track the number of batches\n",
        "        nb_eval_steps += 1\n",
        "    # Report the final accuracy for this validation run.\n",
        "    print(\"  Accuracy: {0:.2f}\".format(eval_accuracy/nb_eval_steps))\n",
        "    print(\"  Validation took: {:}\".format(format_time(time.time() - t0)))\n",
        "print(\"\")\n",
        "print(\"Training complete!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J2_hLmo0pGXJ",
        "outputId": "21ae9453-3e65-4b37-a0ba-f55eab6ef60b"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======== Epoch 1 / 2 ========\n",
            "Training...\n",
            "  Batch    40  of  1,280.    Elapsed: 0:00:14.\n",
            "  Batch    80  of  1,280.    Elapsed: 0:00:28.\n",
            "  Batch   120  of  1,280.    Elapsed: 0:00:42.\n",
            "  Batch   160  of  1,280.    Elapsed: 0:00:56.\n",
            "  Batch   200  of  1,280.    Elapsed: 0:01:11.\n",
            "  Batch   240  of  1,280.    Elapsed: 0:01:26.\n",
            "  Batch   280  of  1,280.    Elapsed: 0:01:41.\n",
            "  Batch   320  of  1,280.    Elapsed: 0:01:55.\n",
            "  Batch   360  of  1,280.    Elapsed: 0:02:10.\n",
            "  Batch   400  of  1,280.    Elapsed: 0:02:25.\n",
            "  Batch   440  of  1,280.    Elapsed: 0:02:40.\n",
            "  Batch   480  of  1,280.    Elapsed: 0:02:54.\n",
            "  Batch   520  of  1,280.    Elapsed: 0:03:09.\n",
            "  Batch   560  of  1,280.    Elapsed: 0:03:24.\n",
            "  Batch   600  of  1,280.    Elapsed: 0:03:39.\n",
            "  Batch   640  of  1,280.    Elapsed: 0:03:53.\n",
            "  Batch   680  of  1,280.    Elapsed: 0:04:08.\n",
            "  Batch   720  of  1,280.    Elapsed: 0:04:23.\n",
            "  Batch   760  of  1,280.    Elapsed: 0:04:38.\n",
            "  Batch   800  of  1,280.    Elapsed: 0:04:53.\n",
            "  Batch   840  of  1,280.    Elapsed: 0:05:07.\n",
            "  Batch   880  of  1,280.    Elapsed: 0:05:22.\n",
            "  Batch   920  of  1,280.    Elapsed: 0:05:37.\n",
            "  Batch   960  of  1,280.    Elapsed: 0:05:51.\n",
            "  Batch 1,000  of  1,280.    Elapsed: 0:06:06.\n",
            "  Batch 1,040  of  1,280.    Elapsed: 0:06:21.\n",
            "  Batch 1,080  of  1,280.    Elapsed: 0:06:36.\n",
            "  Batch 1,120  of  1,280.    Elapsed: 0:06:50.\n",
            "  Batch 1,160  of  1,280.    Elapsed: 0:07:05.\n",
            "  Batch 1,200  of  1,280.    Elapsed: 0:07:20.\n",
            "  Batch 1,240  of  1,280.    Elapsed: 0:07:34.\n",
            "\n",
            "  Average training loss: 1.76\n",
            "  Training epcoh took: 0:07:49\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.24\n",
            "  Validation took: 0:00:21\n",
            "\n",
            "======== Epoch 2 / 2 ========\n",
            "Training...\n",
            "  Batch    40  of  1,280.    Elapsed: 0:00:15.\n",
            "  Batch    80  of  1,280.    Elapsed: 0:00:29.\n",
            "  Batch   120  of  1,280.    Elapsed: 0:00:44.\n",
            "  Batch   160  of  1,280.    Elapsed: 0:00:59.\n",
            "  Batch   200  of  1,280.    Elapsed: 0:01:14.\n",
            "  Batch   240  of  1,280.    Elapsed: 0:01:28.\n",
            "  Batch   280  of  1,280.    Elapsed: 0:01:43.\n",
            "  Batch   320  of  1,280.    Elapsed: 0:01:58.\n",
            "  Batch   360  of  1,280.    Elapsed: 0:02:12.\n",
            "  Batch   400  of  1,280.    Elapsed: 0:02:27.\n",
            "  Batch   440  of  1,280.    Elapsed: 0:02:42.\n",
            "  Batch   480  of  1,280.    Elapsed: 0:02:57.\n",
            "  Batch   520  of  1,280.    Elapsed: 0:03:11.\n",
            "  Batch   560  of  1,280.    Elapsed: 0:03:26.\n",
            "  Batch   600  of  1,280.    Elapsed: 0:03:41.\n",
            "  Batch   640  of  1,280.    Elapsed: 0:03:55.\n",
            "  Batch   680  of  1,280.    Elapsed: 0:04:10.\n",
            "  Batch   720  of  1,280.    Elapsed: 0:04:25.\n",
            "  Batch   760  of  1,280.    Elapsed: 0:04:40.\n",
            "  Batch   800  of  1,280.    Elapsed: 0:04:54.\n",
            "  Batch   840  of  1,280.    Elapsed: 0:05:09.\n",
            "  Batch   880  of  1,280.    Elapsed: 0:05:24.\n",
            "  Batch   920  of  1,280.    Elapsed: 0:05:38.\n",
            "  Batch   960  of  1,280.    Elapsed: 0:05:53.\n",
            "  Batch 1,000  of  1,280.    Elapsed: 0:06:08.\n",
            "  Batch 1,040  of  1,280.    Elapsed: 0:06:23.\n",
            "  Batch 1,080  of  1,280.    Elapsed: 0:06:37.\n",
            "  Batch 1,120  of  1,280.    Elapsed: 0:06:52.\n",
            "  Batch 1,160  of  1,280.    Elapsed: 0:07:07.\n",
            "  Batch 1,200  of  1,280.    Elapsed: 0:07:21.\n",
            "  Batch 1,240  of  1,280.    Elapsed: 0:07:36.\n",
            "\n",
            "  Average training loss: 1.70\n",
            "  Training epcoh took: 0:07:51\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.27\n",
            "  Validation took: 0:00:21\n",
            "\n",
            "Training complete!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Plotting the training loss over epochs\n",
        "import plotly.express as px\n",
        "f = pd.DataFrame(loss_values)\n",
        "f.columns=['Loss']\n",
        "fig = px.line(f, x=f.index, y=f.Loss)\n",
        "fig.update_layout(title='Training loss of the Model',\n",
        "                   xaxis_title='Epoch',\n",
        "                   yaxis_title='Loss')\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "WWhr8QMp7bhY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "outputId": "1888a2d2-b4e6-4c92-e2a0-3e966a6323c7"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-2.18.2.min.js\"></script>                <div id=\"1eb39f8f-a513-417a-8602-d3bf0e4a14af\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"1eb39f8f-a513-417a-8602-d3bf0e4a14af\")) {                    Plotly.newPlot(                        \"1eb39f8f-a513-417a-8602-d3bf0e4a14af\",                        [{\"hovertemplate\":\"index=%{x}<br>Loss=%{y}<extra></extra>\",\"legendgroup\":\"\",\"line\":{\"color\":\"#636efa\",\"dash\":\"solid\"},\"marker\":{\"symbol\":\"circle\"},\"mode\":\"lines\",\"name\":\"\",\"orientation\":\"v\",\"showlegend\":false,\"x\":[0,1],\"xaxis\":\"x\",\"y\":[1.7599638428539037,1.6953037607483565],\"yaxis\":\"y\",\"type\":\"scatter\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"Epoch\"}},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"Loss\"}},\"legend\":{\"tracegroupgap\":0},\"margin\":{\"t\":60},\"title\":{\"text\":\"Training loss of the Model\"}},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('1eb39f8f-a513-417a-8602-d3bf0e4a14af');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bert_prediction_sampler = SequentialSampler(bert_test_dataset)\n",
        "bert_prediction_dataloader = DataLoader(bert_test_dataset, sampler=bert_prediction_sampler, batch_size=batch_size)"
      ],
      "metadata": {
        "id": "rKcZq7oj7hLX"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prediction on test set\n",
        "\n",
        "print('Predicting labels for {:,} test sentences...'.format(len(bert_input_ids_test)))\n",
        "\n",
        "# Put model in evaluation mode\n",
        "bert_model.eval()\n",
        "\n",
        "# Tracking variables \n",
        "predictions , true_labels = [], []"
      ],
      "metadata": {
        "id": "5_2UsuPI7liA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1ed0d64c-5914-4b63-d93c-1ed0b380d1df"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicting labels for 1,267 test sentences...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict \n",
        "for batch in bert_prediction_dataloader:\n",
        "  # Add batch to GPU\n",
        "  batch = tuple(t.to(device) for t in batch)\n",
        "  \n",
        "  # Unpack the inputs from our dataloader\n",
        "  b_input_ids, b_input_mask, b_labels = batch\n",
        "  \n",
        " \n",
        "  with torch.no_grad():\n",
        "      # Forward pass, calculate logit predictions\n",
        "      outputs = bert_model(b_input_ids, token_type_ids=None, \n",
        "                      attention_mask=b_input_mask)\n",
        "\n",
        "  logits = outputs[0]\n",
        "\n",
        "  # Move logits and labels to CPU\n",
        "  logits = logits.detach().cpu().numpy()\n",
        "  label_ids = b_labels.to('cpu').numpy()\n",
        "  \n",
        "  # Store predictions and true labels\n",
        "  predictions.append(logits)\n",
        "  true_labels.append(label_ids)\n",
        "print('    DONE.')"
      ],
      "metadata": {
        "id": "5Mne7r517pVB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7e502a93-9525-4260-ba9d-3b24f0f63833"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    DONE.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predictions_labels = [item for subitem in predictions for item in subitem]\n",
        "\n",
        "predictions_labels = np.argmax(predictions_labels, axis=1).flatten()\n",
        "\n",
        "# Combine the correct labels for each batch into a single list.\n",
        "flat_true_labels = [item for sublist in true_labels for item in sublist]\n",
        "\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "print (classification_report(predictions_labels, flat_true_labels))\n",
        "print(confusion_matrix(flat_true_labels, predictions_labels))"
      ],
      "metadata": {
        "id": "VfRxKRqs7tz_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "09cc1b7f-9124-4522-de33-748c6871b875"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.48      0.28      0.35       429\n",
            "           1       0.03      0.35      0.06        20\n",
            "           2       0.29      0.29      0.29       245\n",
            "           3       0.46      0.23      0.31       526\n",
            "           4       0.00      0.50      0.01         2\n",
            "           5       0.12      0.24      0.16        45\n",
            "\n",
            "    accuracy                           0.26      1267\n",
            "   macro avg       0.23      0.31      0.20      1267\n",
            "weighted avg       0.41      0.26      0.31      1267\n",
            "\n",
            "[[119   3  40  79   0   8]\n",
            " [ 42   7  61  95   0   3]\n",
            " [ 44   7  70 117   0   3]\n",
            " [ 85   2  47 121   1   9]\n",
            " [ 87   1  20  92   1  11]\n",
            " [ 52   0   7  22   0  11]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os"
      ],
      "metadata": {
        "id": "b2qhJUQ7ebbx"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\""
      ],
      "metadata": {
        "id": "BI6ZuAtp82a9"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "# This training code is based on the `run_glue.py` script here:\n",
        "# https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L128\n",
        "# Set the seed value all over the place to make this reproducible.\n",
        "seed_val = 42\n",
        "random.seed(seed_val)\n",
        "np.random.seed(seed_val)\n",
        "torch.manual_seed(seed_val)\n",
        "torch.cuda.manual_seed_all(seed_val)\n",
        "loss_values = []\n",
        "# For each epoch...\n",
        "for epoch_i in range(0, epochs):\n",
        "    \n",
        "    # Training\n",
        "\n",
        "    print(\"\")\n",
        "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
        "    print('Training...')\n",
        "    # Measure how long the training epoch takes.\n",
        "    t0 = time.time()\n",
        "    # Reset the total loss for this epoch.\n",
        "    total_loss = 0\n",
        "    roberta_model.train()\n",
        "    # For each batch of training data...\n",
        "    for step, batch in enumerate(roberta_train_dataloader):\n",
        "        # Report progress after every 40 epochs\n",
        "        if step % 40 == 0 and not step == 0:\n",
        "            # Calculate elapsed time in minutes.\n",
        "            elapsed = format_time(time.time() - t0)\n",
        "            \n",
        "            #Printing current batch and elapsed time\n",
        "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(roberta_train_dataloader), elapsed))\n",
        "        \n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_input_mask = batch[1].to(device)\n",
        "        b_labels = batch[2].to(device)\n",
        "        \n",
        "        roberta_model.zero_grad()        \n",
        "        \n",
        "        outputs = roberta_model(b_input_ids, \n",
        "                    token_type_ids=None, \n",
        "                    attention_mask=b_input_mask, \n",
        "                    labels=b_labels)\n",
        "        \n",
        "        # Model returns tuple, extract loss value from that tuple\n",
        "        loss = outputs[0]\n",
        "        \n",
        "        total_loss += loss.item()\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(roberta_model.parameters(), 1.0)\n",
        "        roberta_optimizer.step()\n",
        "        \n",
        "        roberta_scheduler.step()\n",
        "    # Calculate the average loss over the training data.\n",
        "    avg_train_loss = total_loss / len(roberta_train_dataloader)            \n",
        "    \n",
        "    # Store the loss value for plotting the learning curve.\n",
        "    loss_values.append(avg_train_loss)\n",
        "    print(\"\")\n",
        "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
        "    print(\"  Training epcoh took: {:}\".format(format_time(time.time() - t0)))\n",
        "        \n",
        "    # Validation\n",
        "    \n",
        "    print(\"\")\n",
        "    print(\"Running Validation...\")\n",
        "    t0 = time.time()\n",
        "    # Put the model in evaluation mode    \n",
        "    roberta_model.eval()\n",
        "     \n",
        "    eval_loss, eval_accuracy = 0, 0\n",
        "    nb_eval_steps, nb_eval_examples = 0, 0\n",
        "    # Evaluate data for one epoch\n",
        "    for batch in roberta_validation_dataloader:\n",
        "        \n",
        "        # Add batch to GPU\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        \n",
        "        # Unpack the inputs from our dataloader\n",
        "        b_input_ids, b_input_mask, b_labels = batch\n",
        "        \n",
        "        with torch.no_grad():        \n",
        "           \n",
        "            outputs = roberta_model(b_input_ids, \n",
        "                            token_type_ids=None, \n",
        "                            attention_mask=b_input_mask)\n",
        "        \n",
        "        \n",
        "        logits = outputs[0]\n",
        "        # Move logits and labels to CPU\n",
        "        logits = logits.detach().cpu().numpy()\n",
        "        label_ids = b_labels.to('cpu').numpy()\n",
        "        \n",
        "        # Calculate the accuracy for this batch of test sentences.\n",
        "        tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
        "        \n",
        "        # Accumulate the total accuracy.\n",
        "        eval_accuracy += tmp_eval_accuracy\n",
        "        # Track the number of batches\n",
        "        nb_eval_steps += 1\n",
        "    # Report the final accuracy for this validation run.\n",
        "    print(\"  Accuracy: {0:.2f}\".format(eval_accuracy/nb_eval_steps))\n",
        "    print(\"  Validation took: {:}\".format(format_time(time.time() - t0)))\n",
        "print(\"\")\n",
        "print(\"Training complete!\")"
      ],
      "metadata": {
        "id": "udx6pt5u77Vo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "77c330b2-c22a-4a04-d2e1-dd6bcf266ad0"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======== Epoch 1 / 2 ========\n",
            "Training...\n",
            "  Batch    40  of  1,280.    Elapsed: 0:00:48.\n",
            "  Batch    80  of  1,280.    Elapsed: 0:01:37.\n",
            "  Batch   120  of  1,280.    Elapsed: 0:02:25.\n",
            "  Batch   160  of  1,280.    Elapsed: 0:03:14.\n",
            "  Batch   200  of  1,280.    Elapsed: 0:04:03.\n",
            "  Batch   240  of  1,280.    Elapsed: 0:04:51.\n",
            "  Batch   280  of  1,280.    Elapsed: 0:05:39.\n",
            "  Batch   320  of  1,280.    Elapsed: 0:06:28.\n",
            "  Batch   360  of  1,280.    Elapsed: 0:07:16.\n",
            "  Batch   400  of  1,280.    Elapsed: 0:08:05.\n",
            "  Batch   440  of  1,280.    Elapsed: 0:08:53.\n",
            "  Batch   480  of  1,280.    Elapsed: 0:09:42.\n",
            "  Batch   520  of  1,280.    Elapsed: 0:10:31.\n",
            "  Batch   560  of  1,280.    Elapsed: 0:11:19.\n",
            "  Batch   600  of  1,280.    Elapsed: 0:12:08.\n",
            "  Batch   640  of  1,280.    Elapsed: 0:12:56.\n",
            "  Batch   680  of  1,280.    Elapsed: 0:13:45.\n",
            "  Batch   720  of  1,280.    Elapsed: 0:14:33.\n",
            "  Batch   760  of  1,280.    Elapsed: 0:15:22.\n",
            "  Batch   800  of  1,280.    Elapsed: 0:16:10.\n",
            "  Batch   840  of  1,280.    Elapsed: 0:16:59.\n",
            "  Batch   880  of  1,280.    Elapsed: 0:17:47.\n",
            "  Batch   920  of  1,280.    Elapsed: 0:18:36.\n",
            "  Batch   960  of  1,280.    Elapsed: 0:19:24.\n",
            "  Batch 1,000  of  1,280.    Elapsed: 0:20:13.\n",
            "  Batch 1,040  of  1,280.    Elapsed: 0:21:01.\n",
            "  Batch 1,080  of  1,280.    Elapsed: 0:21:50.\n",
            "  Batch 1,120  of  1,280.    Elapsed: 0:22:38.\n",
            "  Batch 1,160  of  1,280.    Elapsed: 0:23:27.\n",
            "  Batch 1,200  of  1,280.    Elapsed: 0:24:15.\n",
            "  Batch 1,240  of  1,280.    Elapsed: 0:25:04.\n",
            "\n",
            "  Average training loss: 1.78\n",
            "  Training epcoh took: 0:25:52\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.20\n",
            "  Validation took: 0:01:00\n",
            "\n",
            "======== Epoch 2 / 2 ========\n",
            "Training...\n",
            "  Batch    40  of  1,280.    Elapsed: 0:00:49.\n",
            "  Batch    80  of  1,280.    Elapsed: 0:01:37.\n",
            "  Batch   120  of  1,280.    Elapsed: 0:02:26.\n",
            "  Batch   160  of  1,280.    Elapsed: 0:03:14.\n",
            "  Batch   200  of  1,280.    Elapsed: 0:04:03.\n",
            "  Batch   240  of  1,280.    Elapsed: 0:04:51.\n",
            "  Batch   280  of  1,280.    Elapsed: 0:05:40.\n",
            "  Batch   320  of  1,280.    Elapsed: 0:06:28.\n",
            "  Batch   360  of  1,280.    Elapsed: 0:07:17.\n",
            "  Batch   400  of  1,280.    Elapsed: 0:08:05.\n",
            "  Batch   440  of  1,280.    Elapsed: 0:08:54.\n",
            "  Batch   480  of  1,280.    Elapsed: 0:09:42.\n",
            "  Batch   520  of  1,280.    Elapsed: 0:10:31.\n",
            "  Batch   560  of  1,280.    Elapsed: 0:11:19.\n",
            "  Batch   600  of  1,280.    Elapsed: 0:12:08.\n",
            "  Batch   640  of  1,280.    Elapsed: 0:12:56.\n",
            "  Batch   680  of  1,280.    Elapsed: 0:13:45.\n",
            "  Batch   720  of  1,280.    Elapsed: 0:14:33.\n",
            "  Batch   760  of  1,280.    Elapsed: 0:15:22.\n",
            "  Batch   800  of  1,280.    Elapsed: 0:16:10.\n",
            "  Batch   840  of  1,280.    Elapsed: 0:16:59.\n",
            "  Batch   880  of  1,280.    Elapsed: 0:17:47.\n",
            "  Batch   920  of  1,280.    Elapsed: 0:18:36.\n",
            "  Batch   960  of  1,280.    Elapsed: 0:19:24.\n",
            "  Batch 1,000  of  1,280.    Elapsed: 0:20:13.\n",
            "  Batch 1,040  of  1,280.    Elapsed: 0:21:01.\n",
            "  Batch 1,080  of  1,280.    Elapsed: 0:21:50.\n",
            "  Batch 1,120  of  1,280.    Elapsed: 0:22:38.\n",
            "  Batch 1,160  of  1,280.    Elapsed: 0:23:27.\n",
            "  Batch 1,200  of  1,280.    Elapsed: 0:24:15.\n",
            "  Batch 1,240  of  1,280.    Elapsed: 0:25:04.\n",
            "\n",
            "  Average training loss: 1.77\n",
            "  Training epcoh took: 0:25:53\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.19\n",
            "  Validation took: 0:01:00\n",
            "\n",
            "Training complete!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "roberta_prediction_sampler = SequentialSampler(roberta_test_dataset)\n",
        "roberta_prediction_dataloader = DataLoader(roberta_test_dataset, sampler=roberta_prediction_sampler, batch_size=batch_size)"
      ],
      "metadata": {
        "id": "_iX8n8Pslgz3"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prediction on test set\n",
        "\n",
        "print('Predicting labels for {:,} test sentences...'.format(len(bert_input_ids_test)))\n",
        "\n",
        "# Put model in evaluation mode\n",
        "roberta_model.eval()\n",
        "\n",
        "# Tracking variables \n",
        "predictions , true_labels = [], []"
      ],
      "metadata": {
        "id": "tcyb0nI0lr9G",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "85b79e9e-a1cd-422c-829c-902f305d6774"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicting labels for 1,267 test sentences...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict \n",
        "for batch in roberta_prediction_dataloader:\n",
        "  # Add batch to GPU\n",
        "  batch = tuple(t.to(device) for t in batch)\n",
        "  \n",
        "  # Unpack the inputs from our dataloader\n",
        "  b_input_ids, b_input_mask, b_labels = batch\n",
        "\n",
        "  with torch.no_grad():\n",
        "      \n",
        "      outputs = roberta_model(b_input_ids, token_type_ids=None, \n",
        "                      attention_mask=b_input_mask)\n",
        "\n",
        "  logits = outputs[0]\n",
        "\n",
        "  \n",
        "  logits = logits.detach().cpu().numpy()\n",
        "  label_ids = b_labels.to('cpu').numpy()\n",
        "  \n",
        "  # Store predictions and true labels\n",
        "  predictions.append(logits)\n",
        "  true_labels.append(label_ids)\n",
        "print('    DONE.')"
      ],
      "metadata": {
        "id": "9DH7_XzPlxHl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "53b14aa1-0038-4614-f89b-2d388d71be9d"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    DONE.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predictions_labels = [item for subitem in predictions for item in subitem]\n",
        "\n",
        "predictions_labels = np.argmax(predictions_labels, axis=1).flatten()\n",
        "\n",
        "# Combine the correct labels for each batch into a single list.\n",
        "flat_true_labels = [item for sublist in true_labels for item in sublist]\n",
        "\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "print (classification_report(predictions_labels, flat_true_labels))\n",
        "print(confusion_matrix(flat_true_labels, predictions_labels))"
      ],
      "metadata": {
        "id": "SsuWI1dclyP-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5352c4cf-6652-4aaf-b2ee-53befeca0aec"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00         0\n",
            "           1       0.00      0.00      0.00         0\n",
            "           2       0.00      0.00      0.00         0\n",
            "           3       1.00      0.21      0.35      1267\n",
            "           4       0.00      0.00      0.00         0\n",
            "           5       0.00      0.00      0.00         0\n",
            "\n",
            "    accuracy                           0.21      1267\n",
            "   macro avg       0.17      0.03      0.06      1267\n",
            "weighted avg       1.00      0.21      0.35      1267\n",
            "\n",
            "[[  0   0   0 249   0   0]\n",
            " [  0   0   0 208   0   0]\n",
            " [  0   0   0 241   0   0]\n",
            " [  0   0   0 265   0   0]\n",
            " [  0   0   0 212   0   0]\n",
            " [  0   0   0  92   0   0]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning:\n",
            "\n",
            "Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning:\n",
            "\n",
            "Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning:\n",
            "\n",
            "Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "\n"
          ]
        }
      ]
    }
  ]
}