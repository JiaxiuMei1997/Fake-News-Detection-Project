{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "pRFJnwArMhZW"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "data_train = pd.read_csv(\"/content/drive/MyDrive/fake-news/train.tsv\", sep=\"\\t\", header=None)\n",
        "data_valid = pd.read_csv(\"/content/drive/MyDrive/fake-news/valid.tsv\", sep=\"\\t\", header=None)\n",
        "data_test = pd.read_csv(\"/content/drive/MyDrive/fake-news/test.tsv\", sep=\"\\t\", header=None)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_train.head(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 704
        },
        "id": "3P5DfEK7Mq8U",
        "outputId": "97879a74-c036-446d-b25c-09d1df2cdaf8"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "           0            1                                                  2   \\\n",
              "0   2635.json        false  Says the Annies List political group supports ...   \n",
              "1  10540.json    half-true  When did the decline of coal start? It started...   \n",
              "2    324.json  mostly-true  Hillary Clinton agrees with John McCain \"by vo...   \n",
              "3   1123.json        false  Health care reform legislation is likely to ma...   \n",
              "4   9028.json    half-true  The economic turnaround started at the end of ...   \n",
              "\n",
              "                                   3               4                     5   \\\n",
              "0                            abortion    dwayne-bohac  State representative   \n",
              "1  energy,history,job-accomplishments  scott-surovell        State delegate   \n",
              "2                      foreign-policy    barack-obama             President   \n",
              "3                         health-care    blog-posting                   NaN   \n",
              "4                        economy,jobs   charlie-crist                   NaN   \n",
              "\n",
              "         6           7     8     9      10     11    12                   13  \n",
              "0     Texas  republican   0.0   1.0    0.0    0.0   0.0             a mailer  \n",
              "1  Virginia    democrat   0.0   0.0    1.0    1.0   0.0      a floor speech.  \n",
              "2  Illinois    democrat  70.0  71.0  160.0  163.0   9.0               Denver  \n",
              "3       NaN        none   7.0  19.0    3.0    5.0  44.0       a news release  \n",
              "4   Florida    democrat  15.0   9.0   20.0   19.0   2.0  an interview on CNN  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-b2f706fe-9268-4db7-a079-c6d7998256b9\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2635.json</td>\n",
              "      <td>false</td>\n",
              "      <td>Says the Annies List political group supports ...</td>\n",
              "      <td>abortion</td>\n",
              "      <td>dwayne-bohac</td>\n",
              "      <td>State representative</td>\n",
              "      <td>Texas</td>\n",
              "      <td>republican</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>a mailer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>10540.json</td>\n",
              "      <td>half-true</td>\n",
              "      <td>When did the decline of coal start? It started...</td>\n",
              "      <td>energy,history,job-accomplishments</td>\n",
              "      <td>scott-surovell</td>\n",
              "      <td>State delegate</td>\n",
              "      <td>Virginia</td>\n",
              "      <td>democrat</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>a floor speech.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>324.json</td>\n",
              "      <td>mostly-true</td>\n",
              "      <td>Hillary Clinton agrees with John McCain \"by vo...</td>\n",
              "      <td>foreign-policy</td>\n",
              "      <td>barack-obama</td>\n",
              "      <td>President</td>\n",
              "      <td>Illinois</td>\n",
              "      <td>democrat</td>\n",
              "      <td>70.0</td>\n",
              "      <td>71.0</td>\n",
              "      <td>160.0</td>\n",
              "      <td>163.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>Denver</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1123.json</td>\n",
              "      <td>false</td>\n",
              "      <td>Health care reform legislation is likely to ma...</td>\n",
              "      <td>health-care</td>\n",
              "      <td>blog-posting</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>none</td>\n",
              "      <td>7.0</td>\n",
              "      <td>19.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>44.0</td>\n",
              "      <td>a news release</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>9028.json</td>\n",
              "      <td>half-true</td>\n",
              "      <td>The economic turnaround started at the end of ...</td>\n",
              "      <td>economy,jobs</td>\n",
              "      <td>charlie-crist</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Florida</td>\n",
              "      <td>democrat</td>\n",
              "      <td>15.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>19.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>an interview on CNN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b2f706fe-9268-4db7-a079-c6d7998256b9')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-b2f706fe-9268-4db7-a079-c6d7998256b9 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-b2f706fe-9268-4db7-a079-c6d7998256b9');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_test.head(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 739
        },
        "id": "Of-av3O6MtnQ",
        "outputId": "70f3e445-c845-4315-dedd-0d65ae0d5c85"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "           0           1                                                  2   \\\n",
              "0  11972.json        true  Building a wall on the U.S.-Mexico border will...   \n",
              "1  11685.json       false  Wisconsin is on pace to double the number of l...   \n",
              "2  11096.json       false  Says John McCain has done nothing to help the ...   \n",
              "3   5209.json   half-true  Suzanne Bonamici supports a plan that will cut...   \n",
              "4   9524.json  pants-fire  When asked by a reporter whether hes at the ce...   \n",
              "\n",
              "                                                  3   \\\n",
              "0                                        immigration   \n",
              "1                                               jobs   \n",
              "2                    military,veterans,voting-record   \n",
              "3  medicare,message-machine-2012,campaign-adverti...   \n",
              "4  campaign-finance,legal-issues,campaign-adverti...   \n",
              "\n",
              "                                 4                     5          6   \\\n",
              "0                        rick-perry              Governor      Texas   \n",
              "1                 katrina-shankland  State representative  Wisconsin   \n",
              "2                      donald-trump       President-Elect   New York   \n",
              "3                     rob-cornilles            consultant     Oregon   \n",
              "4  state-democratic-party-wisconsin                   NaN  Wisconsin   \n",
              "\n",
              "           7   8    9   10  11  12                            13  \n",
              "0  republican  30   30  42  23  18               Radio interview  \n",
              "1    democrat   2    1   0   0   0             a news conference  \n",
              "2  republican  63  114  51  37  61  comments on ABC's This Week.  \n",
              "3  republican   1    1   3   1   1                  a radio show  \n",
              "4    democrat   5    7   2   2   7                   a web video  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-37c0d5cd-433c-4f9c-85b1-7e698de7e1eb\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>11972.json</td>\n",
              "      <td>true</td>\n",
              "      <td>Building a wall on the U.S.-Mexico border will...</td>\n",
              "      <td>immigration</td>\n",
              "      <td>rick-perry</td>\n",
              "      <td>Governor</td>\n",
              "      <td>Texas</td>\n",
              "      <td>republican</td>\n",
              "      <td>30</td>\n",
              "      <td>30</td>\n",
              "      <td>42</td>\n",
              "      <td>23</td>\n",
              "      <td>18</td>\n",
              "      <td>Radio interview</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>11685.json</td>\n",
              "      <td>false</td>\n",
              "      <td>Wisconsin is on pace to double the number of l...</td>\n",
              "      <td>jobs</td>\n",
              "      <td>katrina-shankland</td>\n",
              "      <td>State representative</td>\n",
              "      <td>Wisconsin</td>\n",
              "      <td>democrat</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>a news conference</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>11096.json</td>\n",
              "      <td>false</td>\n",
              "      <td>Says John McCain has done nothing to help the ...</td>\n",
              "      <td>military,veterans,voting-record</td>\n",
              "      <td>donald-trump</td>\n",
              "      <td>President-Elect</td>\n",
              "      <td>New York</td>\n",
              "      <td>republican</td>\n",
              "      <td>63</td>\n",
              "      <td>114</td>\n",
              "      <td>51</td>\n",
              "      <td>37</td>\n",
              "      <td>61</td>\n",
              "      <td>comments on ABC's This Week.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>5209.json</td>\n",
              "      <td>half-true</td>\n",
              "      <td>Suzanne Bonamici supports a plan that will cut...</td>\n",
              "      <td>medicare,message-machine-2012,campaign-adverti...</td>\n",
              "      <td>rob-cornilles</td>\n",
              "      <td>consultant</td>\n",
              "      <td>Oregon</td>\n",
              "      <td>republican</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>a radio show</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>9524.json</td>\n",
              "      <td>pants-fire</td>\n",
              "      <td>When asked by a reporter whether hes at the ce...</td>\n",
              "      <td>campaign-finance,legal-issues,campaign-adverti...</td>\n",
              "      <td>state-democratic-party-wisconsin</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Wisconsin</td>\n",
              "      <td>democrat</td>\n",
              "      <td>5</td>\n",
              "      <td>7</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>7</td>\n",
              "      <td>a web video</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-37c0d5cd-433c-4f9c-85b1-7e698de7e1eb')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-37c0d5cd-433c-4f9c-85b1-7e698de7e1eb button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-37c0d5cd-433c-4f9c-85b1-7e698de7e1eb');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Viewing sample train data before preprocessing\n",
        "data_valid.head(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 617
        },
        "id": "K9tuR512M6dV",
        "outputId": "ab5d694f-0048-4ed8-f1b2-0c4a8bfbd42e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "           0            1                                                  2   \\\n",
              "0  12134.json  barely-true  We have less Americans working now than in the...   \n",
              "1    238.json   pants-fire  When Obama was sworn into office, he DID NOT u...   \n",
              "2   7891.json        false  Says Having organizations parading as being so...   \n",
              "3   8169.json    half-true     Says nearly half of Oregons children are poor.   \n",
              "4    929.json    half-true  On attacks by Republicans that various program...   \n",
              "\n",
              "                                 3                4   \\\n",
              "0                      economy,jobs   vicky-hartzler   \n",
              "1  obama-birth-certificate,religion      chain-email   \n",
              "2   campaign-finance,congress,taxes  earl-blumenauer   \n",
              "3                           poverty  jim-francesconi   \n",
              "4                  economy,stimulus     barack-obama   \n",
              "\n",
              "                                              5         6           7   8   \\\n",
              "0                            U.S. Representative  Missouri  republican   1   \n",
              "1                                            NaN       NaN        none  11   \n",
              "2                            U.S. representative    Oregon    democrat   0   \n",
              "3  Member of the State Board of Higher Education    Oregon        none   0   \n",
              "4                                      President  Illinois    democrat  70   \n",
              "\n",
              "   9    10   11   12                             13  \n",
              "0   0    1    0    0   an interview with ABC17 News  \n",
              "1  43    8    5  105                            NaN  \n",
              "2   1    1    1    0  a U.S. Ways and Means hearing  \n",
              "3   1    1    1    0             an opinion article  \n",
              "4  71  160  163    9        interview with CBS News  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-2c731735-98b1-440c-9a1c-4019ff748528\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>12134.json</td>\n",
              "      <td>barely-true</td>\n",
              "      <td>We have less Americans working now than in the...</td>\n",
              "      <td>economy,jobs</td>\n",
              "      <td>vicky-hartzler</td>\n",
              "      <td>U.S. Representative</td>\n",
              "      <td>Missouri</td>\n",
              "      <td>republican</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>an interview with ABC17 News</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>238.json</td>\n",
              "      <td>pants-fire</td>\n",
              "      <td>When Obama was sworn into office, he DID NOT u...</td>\n",
              "      <td>obama-birth-certificate,religion</td>\n",
              "      <td>chain-email</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>none</td>\n",
              "      <td>11</td>\n",
              "      <td>43</td>\n",
              "      <td>8</td>\n",
              "      <td>5</td>\n",
              "      <td>105</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>7891.json</td>\n",
              "      <td>false</td>\n",
              "      <td>Says Having organizations parading as being so...</td>\n",
              "      <td>campaign-finance,congress,taxes</td>\n",
              "      <td>earl-blumenauer</td>\n",
              "      <td>U.S. representative</td>\n",
              "      <td>Oregon</td>\n",
              "      <td>democrat</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>a U.S. Ways and Means hearing</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>8169.json</td>\n",
              "      <td>half-true</td>\n",
              "      <td>Says nearly half of Oregons children are poor.</td>\n",
              "      <td>poverty</td>\n",
              "      <td>jim-francesconi</td>\n",
              "      <td>Member of the State Board of Higher Education</td>\n",
              "      <td>Oregon</td>\n",
              "      <td>none</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>an opinion article</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>929.json</td>\n",
              "      <td>half-true</td>\n",
              "      <td>On attacks by Republicans that various program...</td>\n",
              "      <td>economy,stimulus</td>\n",
              "      <td>barack-obama</td>\n",
              "      <td>President</td>\n",
              "      <td>Illinois</td>\n",
              "      <td>democrat</td>\n",
              "      <td>70</td>\n",
              "      <td>71</td>\n",
              "      <td>160</td>\n",
              "      <td>163</td>\n",
              "      <td>9</td>\n",
              "      <td>interview with CBS News</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-2c731735-98b1-440c-9a1c-4019ff748528')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-2c731735-98b1-440c-9a1c-4019ff748528 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-2c731735-98b1-440c-9a1c-4019ff748528');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Below function performs all the required data cleaning and preprocessing steps\n",
        "\n",
        "def data_preprocessing(dataset):\n",
        "  #Creating new column called 'label' with 1 for true and mostly-true values, else 0 i.e. 1=real, 0=fake\n",
        "  dataset['label']=[1 if x==\"true\"or x==\"mostly-true\" else 0 for x in dataset[1]] \n",
        "  #Dropping unwanted columns\n",
        "  dataset = dataset.drop(labels=[0,1,8,9,10,11,12] ,axis=1)\n",
        "  \n",
        "  dataset[\"sentence\"] = dataset[2] #Combining metadata and the text columns into single columns\n",
        "\n",
        "  dataset = dataset.drop([2,3,4,5,6,7,13], axis=1) #dropping metadata columns, as we have merged them into a single column\n",
        "  dataset.dropna() #Dropping if there are still any null values\n",
        "\n",
        "  return dataset"
      ],
      "metadata": {
        "id": "HbfEF8ugM88_"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Applying pre-processing to the raw data - train, valid and test sets\n",
        "data_train = data_preprocessing(data_train)\n",
        "data_valid = data_preprocessing(data_valid)\n",
        "data_test = data_preprocessing(data_test)"
      ],
      "metadata": {
        "id": "LLIHKJDvNYZz"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Sample data after preprocessing\n",
        "data_train.head(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "IySPfErDNlfV",
        "outputId": "3091dd34-45c7-4d9f-9058-d07fc1a30b10"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   label                                           sentence\n",
              "0      0  Says the Annies List political group supports ...\n",
              "1      0  When did the decline of coal start? It started...\n",
              "2      1  Hillary Clinton agrees with John McCain \"by vo...\n",
              "3      0  Health care reform legislation is likely to ma...\n",
              "4      0  The economic turnaround started at the end of ..."
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-dd297cf5-4108-4237-ba98-d1b18bd54fcf\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "      <th>sentence</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>Says the Annies List political group supports ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>When did the decline of coal start? It started...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>Hillary Clinton agrees with John McCain \"by vo...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>Health care reform legislation is likely to ma...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>The economic turnaround started at the end of ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-dd297cf5-4108-4237-ba98-d1b18bd54fcf')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-dd297cf5-4108-4237-ba98-d1b18bd54fcf button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-dd297cf5-4108-4237-ba98-d1b18bd54fcf');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Analyzing length of sentences in training data to decide on MAX_LENGTH variable, which is required for BERT and RoBERTa\n",
        "\n",
        "sent_len = []\n",
        "for sent in data_train['sentence']:\n",
        "  sent_len.append(len(sent))\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig = plt.figure(figsize =(10, 7))\n",
        "plt.boxplot(sent_len)\n",
        "plt.show()\n",
        "\n",
        "sent_len = [i for i in sent_len if i<=500] #Excluding the outliers\n",
        "fig2 = plt.figure(figsize =(10, 7))\n",
        "plt.hist(sent_len, 5)\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 839
        },
        "id": "ndjgbH87No1d",
        "outputId": "5f3980fd-d7c9-463f-b924-36a9caa53a2b"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 720x504 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAlwAAAGbCAYAAAARGU4hAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAXe0lEQVR4nO3dX4zl5X3f8c/Xs2u2bv54KVtM+VNQStvBIwWjI+wqe+FNFBtbSDhSZbEXMbJGIkL2KBG5KM5c4CRdlEpNLHnlmBItMq7SoahJZGShupROZY2EbQabmoWt5a3/CFYYNgHbqa1dzW6fXsxvyeDsssvuPHOGOa+XNJpznt/vnPmem9Fb5zy/mWqtBQCAft4y7gEAALY6wQUA0JngAgDoTHABAHQmuAAAOts27gFezyWXXNKuvvrqcY8BAHBWTz755F+31nad7timDq6rr746y8vL4x4DAOCsqur7ZzrmI0UAgM4EFwBAZ4ILAKAzwQUA0JngAgDoTHABAHQmuAAAOhNcAACdCS4AgM4EFwBAZ4ILAKAzwQUA0JngAgDoTHABE2NhYSEzMzOZmprKzMxMFhYWxj0SMCG2jXsAgI2wsLCQ+fn5HDhwILt3787S0lJmZ2eTJHv37h3zdMBWV621cc9wRqPRqC0vL497DGALmJmZyf79+7Nnz55X1xYXFzM3N5eDBw+OcTJgq6iqJ1tro9MeE1zAJJiamsqxY8eyffv2V9dWVlayY8eOnDx5coyTAVvF6wWXPVzARJiens7S0tJr1paWljI9PT2miYBJIriAiTA/P5/Z2dksLi5mZWUli4uLmZ2dzfz8/LhHAyaATfPARDi1MX5ubi6HDh3K9PR09u3bZ8M8sCHs4QIAWAf2cAEAjJHgAgDoTHABAHQmuAAAOhNcAACdCS4AgM4EFwBAZ4ILAKAzwQUA0JngAgDoTHABAHQmuAAAOjtrcFXVjqr6WlX9r6p6pqp+f1i/pqq+WlWHq+o/V9Vbh/WLhvuHh+NXr3muTwzr36qq9/d6UQAAm8m5vMN1PMmvttZ+Ocn1SW6qqvck+XdJPtVa+2dJXkkyO5w/m+SVYf1Tw3mpquuS3JrknUluSvKnVTW1ni8GAGAzOmtwtVX/d7i7ffhqSX41yX8Z1h9I8qHh9i3D/QzHf62qalh/sLV2vLX23SSHk9y4Lq8CAGATO6c9XFU1VVVPJXkpyaNJ/k+SH7bWTgynPJ/k8uH25UmeS5Lh+I+S/KO166d5zNqfdXtVLVfV8tGjR9/4KwIA2GTOKbhaaydba9cnuSKr70r9y14Dtdbua62NWmujXbt29foxAAAb5g1dpdha+2GSxST/Ksnbq2rbcOiKJEeG20eSXJkkw/FfTPI3a9dP8xgAgC3rXK5S3FVVbx9u/4Mkv57kUFbD618Pp92W5AvD7YeH+xmO/4/WWhvWbx2uYrwmybVJvrZeLwQAYLPadvZTclmSB4YrCt+S5KHW2her6tkkD1bVv03yjSQHhvMPJPmPVXU4yctZvTIxrbVnquqhJM8mOZHkY621k+v7cgAANp9affNpcxqNRm15eXncYwAAnFVVPdlaG53umL80DwDQmeACAOhMcAEAdCa4AAA6E1wAAJ0JLgCAzgQXAEBnggsAoDPBBQDQmeACAOhMcAEAdCa4AAA6E1wAAJ0JLgCAzgQXAEBnggsAoDPBBQDQmeACAOhMcAEAdCa4AAA6E1wAAJ0JLgCAzgQXAEBnggsAoDPBBQDQmeACAOhMcAEAdCa4AAA6E1wAAJ0JLgCAzgQXAEBnggsAoDPBBQDQmeACAOhMcAEAdCa4AAA6E1wAAJ0JLgCAzgQXAEBnggsAoDPBBQDQmeACAOhMcAEAdCa4AAA6E1wAAJ0JLgCAzs4aXFV1ZVUtVtWzVfVMVf32sP7JqjpSVU8NXx9c85hPVNXhqvpWVb1/zfpNw9rhqrqrz0sCANhctp3DOSeS/G5r7etV9fNJnqyqR4djn2qt/fu1J1fVdUluTfLOJP8kyX+vqn8+HP5Mkl9P8nySJ6rq4dbas+vxQgAANquzBldr7YUkLwy3/7aqDiW5/HUeckuSB1trx5N8t6oOJ7lxOHa4tfadJKmqB4dzBRcAsKW9oT1cVXV1kncl+eqw9PGq+mZV3V9VO4e1y5M8t+Zhzw9rZ1r/2Z9xe1UtV9Xy0aNH38h4AACb0jkHV1X9XJK/SPI7rbUfJ/lskl9Kcn1W3wH74/UYqLV2X2tt1Fob7dq1az2eEgBgrM5lD1eqantWY+vPW2t/mSSttRfXHP+zJF8c7h5JcuWah18xrOV11gEAtqxzuUqxkhxIcqi19idr1i9bc9pvJDk43H44ya1VdVFVXZPk2iRfS/JEkmur6pqqemtWN9Y/vD4vAwBg8zqXd7h+JclvJnm6qp4a1n4vyd6quj5JS/K9JL+VJK21Z6rqoaxuhj+R5GOttZNJUlUfT/KlJFNJ7m+tPbOOrwUAYFOq1tq4Zzij0WjUlpeXxz0GAMBZVdWTrbXR6Y75S/MAAJ0JLgCAzgQXAEBnggsAoDPBBQDQmeACAOhMcAEAdCa4AAA6E1wAAJ0JLgCAzgQXAEBnggsAoDPBBQDQmeACAOhMcAEAdCa4AAA6E1wAAJ0JLgCAzgQXAEBnggsAoDPBBQDQmeACAOhMcAEAdCa4AAA6E1wAAJ0JLgCAzgQXAEBnggsAoDPBBQDQmeACAOhMcAEAdCa4AAA6E1wAAJ0JLgCAzgQXMDEWFhYyMzOTqampzMzMZGFhYdwjARNi27gHANgICwsLmZ+fz4EDB7J79+4sLS1ldnY2SbJ3794xTwdsddVaG/cMZzQajdry8vK4xwC2gJmZmezfvz979ux5dW1xcTFzc3M5ePDgGCcDtoqqerK1NjrtMcEFTIKpqakcO3Ys27dvf3VtZWUlO3bsyMmTJ8c4GbBVvF5w2cMFTITp6eksLS29Zm1paSnT09NjmgiYJIILmAjz8/OZnZ3N4uJiVlZWsri4mNnZ2czPz497NGAC2DQPTIRTG+Pn5uZy6NChTE9PZ9++fTbMAxvCHi4AgHVgDxcAwBgJLgCAzgQXAEBnZw2uqrqyqhar6tmqeqaqfntYv7iqHq2qbw/fdw7rVVWfrqrDVfXNqrphzXPdNpz/7aq6rd/LAgDYPM7lHa4TSX63tXZdkvck+VhVXZfkriSPtdauTfLYcD9JPpDk2uHr9iSfTVYDLcndSd6d5MYkd5+KNACAreyswdVae6G19vXh9t8mOZTk8iS3JHlgOO2BJB8abt+S5PNt1VeSvL2qLkvy/iSPttZebq29kuTRJDet66sBANiE3tAerqq6Osm7knw1yaWttReGQz9Iculw+/Ikz6152PPD2pnWf/Zn3F5Vy1W1fPTo0TcyHgDApnTOwVVVP5fkL5L8Tmvtx2uPtdU/5rUuf9CrtXZfa23UWhvt2rVrPZ4SAGCszim4qmp7VmPrz1trfzksvzh8VJjh+0vD+pEkV655+BXD2pnWAQC2tHO5SrGSHEhyqLX2J2sOPZzk1JWGtyX5wpr1jwxXK74nyY+Gjx6/lOR9VbVz2Cz/vmENAGBLO5f/pfgrSX4zydNV9dSw9ntJ/ijJQ1U1m+T7ST48HHskyQeTHE7y0yQfTZLW2stV9YdJnhjO+4PW2svr8ioAADYx/0sRAGAd+F+KAABjJLgAADoTXAAAnQkuAIDOBBcAQGeCCwCgM8EFANCZ4AIA6ExwAQB0JrgAADoTXAAAnQkuAIDOBBcAQGeCCwCgM8EFANCZ4AIA6ExwAQB0JrgAADoTXAAAnQkuAIDOBBcAQGeCCwCgM8EFANCZ4AIA6ExwAQB0JrgAADoTXAAAnQkuAIDOBBcAQGeCCwCgM8EFANCZ4AIA6ExwAQB0JrgAADoTXAAAnQkuAIDOBBcwMRYWFjIzM5OpqanMzMxkYWFh3CMBE2LbuAcA2AgLCwuZn5/PgQMHsnv37iwtLWV2djZJsnfv3jFPB2x11Vob9wxnNBqN2vLy8rjHALaAmZmZ7N+/P3v27Hl1bXFxMXNzczl48OAYJwO2iqp6srU2Ou0xwQVMgqmpqRw7dizbt29/dW1lZSU7duzIyZMnxzgZsFW8XnDZwwVMhOnp6SwtLb1mbWlpKdPT02OaCJgkgguYCPPz85mdnc3i4mJWVlayuLiY2dnZzM/Pj3s0YALYNA9MhFMb4+fm5nLo0KFMT09n3759NswDG8IeLgCAdWAPFwDAGJ01uKrq/qp6qaoOrln7ZFUdqaqnhq8Prjn2iao6XFXfqqr3r1m/aVg7XFV3rf9LAQDYnM7lHa7PJbnpNOufaq1dP3w9kiRVdV2SW5O8c3jMn1bVVFVNJflMkg8kuS7J3uFcAIAt76yb5ltrX66qq8/x+W5J8mBr7XiS71bV4SQ3DscOt9a+kyRV9eBw7rNveGIAgDeZC9nD9fGq+ubwkePOYe3yJM+tOef5Ye1M6wAAW975Btdnk/xSkuuTvJDkj9droKq6vaqWq2r56NGj6/W0AABjc17B1Vp7sbV2srX2/5L8Wf7uY8MjSa5cc+oVw9qZ1k/33Pe11kattdGuXbvOZzwAgE3lvIKrqi5bc/c3kpy6gvHhJLdW1UVVdU2Sa5N8LckTSa6tqmuq6q1Z3Vj/8PmPDQDw5nHWTfNVtZDkvUkuqarnk9yd5L1VdX2SluR7SX4rSVprz1TVQ1ndDH8iycdaayeH5/l4ki8lmUpyf2vtmXV/NQAAm5C/NA8AsA78pXkAgDESXAAAnQkuAIDOBBcAQGeCCwCgM8EFANCZ4AIA6ExwAQB0JrgAADoTXAAAnQkuAIDOBBcAQGeCCwCgM8EFANCZ4AIA6ExwAQB0JrgAADoTXAAAnQkuAIDOBBcAQGeCCwCgM8EFANCZ4AIA6ExwAQB0JrgAADoTXAAAnQkuAIDOBBcAQGeCCwCgM8EFANCZ4AIA6ExwAQB0JrgAADoTXAAAnQkuAIDOBBcAQGeCCwCgM8EFANCZ4AIA6ExwAQB0JrgAADoTXAAAnQkuAIDOBBcAQGeCCwCgM8EFANDZWYOrqu6vqpeq6uCatYur6tGq+vbwfeewXlX16ao6XFXfrKob1jzmtuH8b1fVbX1eDgDA5nMu73B9LslNP7N2V5LHWmvXJnlsuJ8kH0hy7fB1e5LPJquBluTuJO9OcmOSu09FGgDAVnfW4GqtfTnJyz+zfEuSB4bbDyT50Jr1z7dVX0ny9qq6LMn7kzzaWnu5tfZKkkfz9yMOAGBLOt89XJe21l4Ybv8gyaXD7cuTPLfmvOeHtTOt/z1VdXtVLVfV8tGjR89zPACAzeOCN8231lqStg6znHq++1pro9baaNeuXev1tAAAY3O+wfXi8FFhhu8vDetHkly55rwrhrUzrQMAbHnnG1wPJzl1peFtSb6wZv0jw9WK70nyo+Gjxy8leV9V7Rw2y79vWAMA2PK2ne2EqlpI8t4kl1TV81m92vCPkjxUVbNJvp/kw8PpjyT5YJLDSX6a5KNJ0lp7uar+MMkTw3l/0Fr72Y34AABbUq1uwdqcRqNRW15eHvcYAABnVVVPttZGpzvmL80DAHQmuAAAOhNcAACdCS4AgM4EFwBAZ4ILAKAzwQUA0JngAgDoTHABAHQmuAAAOhNcAACdCS4AgM4EFwBAZ4ILAKAzwQUA0JngAgDoTHABAHQmuAAAOhNcAACdCS4AgM4EFwBAZ4ILAKAzwQUA0JngAgDoTHABAHQmuAAAOhNcAACdCS4AgM4EFwBAZ4ILAKAzwQUA0JngAgDoTHABAHQmuAAAOhNcAACdCS4AgM4EFwBAZ4ILAKAzwQUA0JngAgDoTHABAHQmuICJsbCwkJmZmUxNTWVmZiYLCwvjHgmYENvGPQDARlhYWMj8/HwOHDiQ3bt3Z2lpKbOzs0mSvXv3jnk6YKur1tq4Zzij0WjUlpeXxz0GsAXMzMxk//792bNnz6tri4uLmZuby8GDB8c4GbBVVNWTrbXRaY8JLmASTE1N5dixY9m+ffuraysrK9mxY0dOnjw5xsmAreL1guuC9nBV1feq6umqeqqqloe1i6vq0ar69vB957BeVfXpqjpcVd+sqhsu5GcDvBHT09NZWlp6zdrS0lKmp6fHNBEwSdZj0/ye1tr1a4ruriSPtdauTfLYcD9JPpDk2uHr9iSfXYefDXBO5ufnMzs7m8XFxaysrGRxcTGzs7OZn58f92jABOixaf6WJO8dbj+Q5H8m+TfD+ufb6meYX6mqt1fVZa21FzrMAPAapzbGz83N5dChQ5mens6+fftsmAc2xIUGV0vy36qqJfkPrbX7kly6JqJ+kOTS4fblSZ5b89jnh7XXBFdV3Z7Vd8By1VVXXeB4AH9n7969AgsYiwsNrt2ttSNV9Y+TPFpV/3vtwdZaG2LsnA3Rdl+yumn+AucDABi7C9rD1Vo7Mnx/KclfJbkxyYtVdVmSDN9fGk4/kuTKNQ+/YlgDANjSzju4quofVtXPn7qd5H1JDiZ5OMltw2m3JfnCcPvhJB8ZrlZ8T5If2b8FAEyCC/lI8dIkf1VVp57nP7XW/mtVPZHkoaqaTfL9JB8ezn8kyQeTHE7y0yQfvYCfDQDwpnHewdVa+06SXz7N+t8k+bXTrLckHzvfnwcA8Gbln1cDAHQmuAAAOhNcAACdCS5gYszNzWXHjh2pquzYsSNzc3PjHgmYEIILmAhzc3O59957c8899+QnP/lJ7rnnntx7772iC9gQtXrx4OY0Go3a8vLyuMcAtoAdO3ZkNBpleXk5x48fz0UXXfTq/WPHjo17PGALqKonW2uj0x3zDhcwEY4fP57HH3/8Ne9wPf744zl+/Pi4RwMmgOACJsbNN9+cO++8M29729ty55135uabbx73SMCEuNB/Xg3wpvHFL34x73jHO/Liiy/m0ksvzdGjR8c9EjAhBBcwEbZtW/119+KLL776fdu2bXnLW7zRD/TnNw0wES666KKcOHEid9xxR374wx/mjjvuyIkTJ3LRRReNezRgArhKEZgIVZUbbrgh3/jGN9JaS1XlXe96V77+9a9nM/8eBN48XKUIkOTpp59+Na5aa3n66afHPBEwKQQXMDFWVlZSVUlW3/FaWVkZ80TApBBcwERZ+w4XwEYRXAAAnQkuAIDOBBcAQGeCCwCgM8EFANCZ4AIA6ExwAQB0JrgAADoTXAAAnQkuAIDOto17AICzOfX/Dzf78/t3QcCZCC5g01uPkHm9qBJKQG8+UgQA6ExwARPhTO9ieXcL2Ag+UgQmxqm4qiqhBWwo73ABAHQmuAAAOhNcAACdCS4AgM5smgfW1cUXX5xXXnll3GOcVe8/proedu7cmZdffnncYwDrQHAB6+qVV15xBeA6eTNEIXBufKQIANCZ4AIA6MxHisC6anf/QvLJXxz3GFtCu/sXxj0CsE4EF7Cu6vd/bA/XOqmqtE+OewpgPfhIEQCgM+9wAevO1XXrY+fOneMeAVgnggtYV2+GjxP982pgo/lIEQCgM8EFANCZ4AIA6GzDg6uqbqqqb1XV4aq6a6N/PgDARtvQ4KqqqSSfSfKBJNcl2VtV123kDAAAG22jr1K8Mcnh1tp3kqSqHkxyS5JnN3gO4E2kx5+Z6PGcrnwEzmSjg+vyJM+tuf98knevPaGqbk9ye5JcddVVGzcZsGkJGeDNbtNtmm+t3ddaG7XWRrt27Rr3OAAAF2yjg+tIkivX3L9iWAMA2LI2OrieSHJtVV1TVW9NcmuShzd4BgCADbWhe7haayeq6uNJvpRkKsn9rbVnNnIGAICNtuH/S7G19kiSRzb65wIAjMum2zQPALDVCC4AgM4EFwBAZ4ILAKAzwQUA0JngAgDoTHABAHQmuAAAOhNcAACdCS4AgM6qtTbuGc6oqo4m+f645wC2nEuS/PW4hwC2nH/aWtt1ugObOrgAeqiq5dbaaNxzAJPDR4oAAJ0JLgCAzgQXMInuG/cAwGSxhwsAoDPvcAEAdCa4AAA6E1zAxKiq+6vqpao6OO5ZgMkiuIBJ8rkkN417CGDyCC5gYrTWvpzk5XHPAUwewQUA0JngAgDoTHABAHQmuAAAOhNcwMSoqoUkjyf5F1X1fFXNjnsmYDL41z4AAJ15hwsAoDPBBQDQmeACAOhMcAEAdCa4AAA6E1wAAJ0JLgCAzv4/w9qURMGE+kwAAAAASUVORK5CYII=\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 720x504 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ZgJKrT8OGhb",
        "outputId": "11dfd317-a52d-4bdb-e14a-74335ee64f45"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.13.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.8.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.13.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.11.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.10.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.9.24)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#importing required packages  \n",
        "from transformers import (\n",
        "    BertForSequenceClassification,    \n",
        "    BertTokenizer,\n",
        "    RobertaForSequenceClassification,\n",
        "    RobertaTokenizer,\n",
        "    AdamW)"
      ],
      "metadata": {
        "id": "nnjNERL0OIrM"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading BERT base model\n",
        "bert_model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", #Using BERT base model with an uncased vocab.\n",
        "                                                                num_labels = 2, #number of output labels - 0,1 (binary classification)\n",
        "                                                                output_attentions = False, #model doesnt return attention weights\n",
        "                                                                output_hidden_states = False #model doesnt return hidden states\n",
        "                                                          )\n",
        "#BERT tokenizer\n",
        "bert_tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\", do_lower_case=True)\n",
        "\n",
        "# Tell pytorch to run this model on the GPU.\n",
        "bert_model.cuda()\n",
        "\n",
        "# Loading RoBERTa base model\n",
        "roberta_model = RobertaForSequenceClassification.from_pretrained(\"roberta-base\", #RoBERTa base model\n",
        "                                                                    num_labels = 2,  #number of output labels - 0,1 (binary classification)\n",
        "                                                                    output_attentions = False,  #model doesnt return attention weights\n",
        "                                                                    output_hidden_states = False #model doesnt return hidden states\n",
        "                                                                )\n",
        "#RoBERTa tokenizer\n",
        "roberta_tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\", do_lower_case=True)\n",
        "\n",
        "# Tell pytorch to run this model on the GPU.\n",
        "roberta_model.cuda()\n",
        "\n",
        "print(' Base models loaded') "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Ozl9LYbOL67",
        "outputId": "dc234146-aa73-41f5-c563-7879b85885d4"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'roberta.pooler.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'roberta.pooler.dense.bias', 'lm_head.layer_norm.weight']\n",
            "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Base models loaded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the original sentence.\n",
        "print(' Original: ', data_train[\"sentence\"][0])\n",
        "\n",
        "# Split the sentence into tokens - BERT\n",
        "print('Tokenized BERT: ', bert_tokenizer.tokenize(data_train[\"sentence\"][0]))\n",
        "\n",
        "# Mapping tokens to token IDs - BERT\n",
        "print('Token IDs BERT: ', bert_tokenizer.convert_tokens_to_ids(bert_tokenizer.tokenize(data_train[\"sentence\"][0])))\n",
        "\n",
        "# Split the sentence into tokens -RoBERTa\n",
        "print('Tokenized RoBERT: ', roberta_tokenizer.tokenize(data_train[\"sentence\"][0]))\n",
        "\n",
        "# Mapping tokens to token IDs - RoBERTa\n",
        "print('Token IDs RoBERTa: ', roberta_tokenizer.convert_tokens_to_ids(roberta_tokenizer.tokenize(data_train[\"sentence\"][0])))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kUO9kODaOV5M",
        "outputId": "76028539-1116-471e-9e63-91511c87670c"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Original:  Says the Annies List political group supports third-trimester abortions on demand.\n",
            "Tokenized BERT:  ['says', 'the', 'annie', '##s', 'list', 'political', 'group', 'supports', 'third', '-', 'trim', '##ester', 'abortion', '##s', 'on', 'demand', '.']\n",
            "Token IDs BERT:  [2758, 1996, 8194, 2015, 2862, 2576, 2177, 6753, 2353, 1011, 12241, 20367, 11324, 2015, 2006, 5157, 1012]\n",
            "Tokenized RoBERT:  ['S', 'ays', 'Ġthe', 'ĠAnn', 'ies', 'ĠList', 'Ġpolitical', 'Ġgroup', 'Ġsupports', 'Ġthird', '-', 'tr', 'imester', 'Ġabortions', 'Ġon', 'Ġdemand', '.']\n",
            "Token IDs RoBERTa:  [104, 4113, 5, 3921, 918, 9527, 559, 333, 4548, 371, 12, 4328, 38417, 17600, 15, 1077, 4]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#assigning sentences and labels to separate variables\n",
        "sentences = data_train[\"sentence\"].values\n",
        "labels = data_train[\"label\"].values"
      ],
      "metadata": {
        "id": "mP7TK4BjO6Je"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch"
      ],
      "metadata": {
        "id": "HYav1wPqPL8L"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Below function performs tokenization process as required by bert and roberta models, for a given dataset\n",
        "def bert_robert_tokenization(dataset):\n",
        "  sentences = dataset[\"sentence\"].values\n",
        "  labels = dataset[\"label\"].values\n",
        "  max_length = 512\n",
        "\n",
        "  # Tokenize all of the sentences and map the tokens to thier word IDs.\n",
        "  bert_input_ids = []\n",
        "  bert_attention_masks = []\n",
        "  roberta_input_ids = []\n",
        "  roberta_attention_masks = []\n",
        "\n",
        "  sentence_ids = []\n",
        "  counter = 0\n",
        "\n",
        "  # For every sentence...\n",
        "  for sent in sentences:\n",
        "      #encode_plus function will encode the sentences as required by model, including tokenization process and mapping token ids\n",
        "      bert_encoded_dict = bert_tokenizer.encode_plus(\n",
        "                          str(sent),        #sentence              \n",
        "                          add_special_tokens = True, # Add '[CLS]' and '[SEP]' tokens \n",
        "                          max_length = 512,     #Since we have seen from our analysis that majority of sentences have length less than 300.    \n",
        "                          pad_to_max_length = True,    # Pad sentences to 256 length  if the length of sentence is less than max_length\n",
        "                          return_attention_mask = True,   # Create attention mask\n",
        "                          truncation = True,  # truncate sentences to 256 length  if the length of sentence is greater than max_length\n",
        "                          return_tensors = 'pt',     # Return pytorch tensors.\n",
        "                    )\n",
        "      \n",
        "      roberta_encoded_dict = roberta_tokenizer.encode_plus(\n",
        "                          str(sent),        #sentence\n",
        "                          add_special_tokens = True, # Add '[CLS]' and '[SEP]' tokens \n",
        "                          max_length = 512,        #Since we have seen from our analysis that majority of sentences have length less than 300.   \n",
        "                          pad_to_max_length = True,     # Pad sentences to 256 length  if the length of sentence is less than max_length\n",
        "                          return_attention_mask = True,   # Create attention mask\n",
        "                          truncation = True,   # truncate sentences to 256 length  if the length of sentence is greater than max_length\n",
        "                          return_tensors = 'pt',     # Return pytorch tensors.\n",
        "                    )\n",
        "      \n",
        "    \n",
        "      # Add the encoded sentence to the list.    \n",
        "      bert_input_ids.append(bert_encoded_dict['input_ids'])\n",
        "      roberta_input_ids.append(roberta_encoded_dict['input_ids'])\n",
        "      \n",
        "      \n",
        "      # Add attention mask to the list \n",
        "      bert_attention_masks.append(bert_encoded_dict['attention_mask'])\n",
        "      roberta_attention_masks.append(roberta_encoded_dict['attention_mask'])\n",
        "      \n",
        "      \n",
        "      # collecting sentence_ids\n",
        "      sentence_ids.append(counter)\n",
        "      counter  = counter + 1\n",
        "      \n",
        "      \n",
        "      \n",
        "  # Convert the lists into tensors.\n",
        "  bert_input_ids = torch.cat(bert_input_ids, dim=0)\n",
        "  bert_attention_masks = torch.cat(bert_attention_masks, dim=0)\n",
        "\n",
        "  roberta_input_ids = torch.cat(roberta_input_ids, dim=0)\n",
        "  roberta_attention_masks = torch.cat(roberta_attention_masks, dim=0)\n",
        "\n",
        "\n",
        "  labels = torch.tensor(labels)\n",
        "  sentence_ids = torch.tensor(sentence_ids)\n",
        "\n",
        "  return {\"Bert\":[bert_input_ids, bert_attention_masks, labels], \"Roberta\":[roberta_input_ids, roberta_attention_masks, labels]}"
      ],
      "metadata": {
        "id": "J_I2NP7IPT70"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import TensorDataset, random_split\n",
        "# function to seed the script globally\n",
        "torch.manual_seed(0)\n",
        "\n",
        "#tokenizing train set\n",
        "token_dict_train = bert_robert_tokenization(data_train)\n",
        "\n",
        "bert_input_ids,bert_attention_masks,labels = token_dict_train[\"Bert\"]\n",
        "roberta_input_ids, roberta_attention_masks, labels = token_dict_train[\"Roberta\"]\n",
        "\n",
        "#tokenizing validation set\n",
        "token_dict_valid = bert_robert_tokenization(data_valid)\n",
        "\n",
        "bert_input_ids_valid,bert_attention_masks_valid,labels_valid = token_dict_valid[\"Bert\"]\n",
        "roberta_input_ids_valid, roberta_attention_masks_valid, labels_valid = token_dict_valid[\"Roberta\"]\n",
        "\n",
        "#tokenizing test set\n",
        "token_dict_test = bert_robert_tokenization(data_test)\n",
        "\n",
        "bert_input_ids_test,bert_attention_masks_test,labels_test = token_dict_test[\"Bert\"]\n",
        "roberta_input_ids_test, roberta_attention_masks_test, labels_test = token_dict_test[\"Roberta\"]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bo_OKSuxPjyn",
        "outputId": "d82fa932-c22f-401b-d794-83aa6d265700"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2310: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Combine the training inputs into a TensorDataset.\n",
        "bert_train_dataset = TensorDataset( bert_input_ids, bert_attention_masks, labels) \n",
        "roberta_train_dataset = TensorDataset(roberta_input_ids, roberta_attention_masks, labels)\n",
        "\n",
        "# Combine the validation inputs into a TensorDataset.\n",
        "bert_val_dataset = TensorDataset(bert_input_ids_valid,bert_attention_masks_valid,labels_valid)\n",
        "roberta_val_dataset = TensorDataset(roberta_input_ids_valid, roberta_attention_masks_valid, labels_valid)\n",
        "\n",
        "# Combine the test inputs into a TensorDataset.\n",
        "\n",
        "bert_test_dataset = TensorDataset(bert_input_ids_test,bert_attention_masks_test,labels_test)\n",
        "roberta_test_dataset = TensorDataset(roberta_input_ids_test, roberta_attention_masks_test, labels_test)"
      ],
      "metadata": {
        "id": "p9ZN-SeJPmJM"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "batch_size = 32\n",
        "\n",
        "# Create the DataLoaders for our training - Loads the data randomly in batches of size 32\n",
        "bert_train_dataloader = DataLoader(\n",
        "            bert_train_dataset,  # The training samples.\n",
        "            sampler = RandomSampler(bert_train_dataset), # Select batches randomly\n",
        "            batch_size = batch_size # Trains with this batch size.\n",
        "        )\n",
        "\n",
        "roberta_train_dataloader = DataLoader(\n",
        "            roberta_train_dataset,  # The training samples.\n",
        "            sampler = RandomSampler(roberta_train_dataset), # Select batches randomly\n",
        "            batch_size = batch_size # Trains with this batch size.\n",
        "        )\n",
        "\n",
        "# Create the DataLoaders for our validation - Loads the data in batches of size 32\n",
        "bert_validation_dataloader = DataLoader(\n",
        "            bert_val_dataset, # The validation samples.\n",
        "            sampler = SequentialSampler(bert_val_dataset), # Pull out batches sequentially.\n",
        "            batch_size = batch_size # Evaluate with this batch size.\n",
        "        )\n",
        "\n",
        "\n",
        "roberta_validation_dataloader = DataLoader(\n",
        "            roberta_val_dataset, # The validation samples.\n",
        "            sampler = SequentialSampler(roberta_val_dataset), # Pull out batches sequentially.\n",
        "            batch_size = batch_size # Evaluate with this batch size.\n",
        "        )"
      ],
      "metadata": {
        "id": "IguU6VGQPqbH"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# optimizers - AdamW\n",
        "# here, i have used default learning rate and epsilon values for both BERT and RoBERTa\n",
        "bert_optimizer = AdamW(bert_model.parameters(),\n",
        "                  lr = 5e-5, \n",
        "                  eps = 1e-8 \n",
        "                )\n",
        "\n",
        "roberta_optimizer = AdamW(roberta_model.parameters(),\n",
        "                  lr = 5e-5, \n",
        "                  eps = 1e-8 \n",
        "                )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_maQajHEPvVQ",
        "outputId": "ddfa8ee6-c603-4f70-8425-7a9e306c3dc7"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import get_linear_schedule_with_warmup\n",
        "\n",
        "# Number of training epochs\n",
        "epochs = 2\n",
        "\n",
        "# Total number of training steps is [number of batches] x [number of epochs]\n",
        "total_steps = len(bert_train_dataloader) * epochs\n",
        "\n",
        "# Create the learning rate scheduler.\n",
        "bert_scheduler = get_linear_schedule_with_warmup(bert_optimizer, \n",
        "                                            num_warmup_steps = 0, \n",
        "                                            num_training_steps = total_steps)\n",
        "\n",
        "roberta_scheduler = get_linear_schedule_with_warmup(roberta_optimizer, \n",
        "                                            num_warmup_steps = 0,\n",
        "                                            num_training_steps = total_steps)"
      ],
      "metadata": {
        "id": "PWZkXT4ePyMF"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Function to calculate the accuracy of our predictions vs labels\n",
        "def flat_accuracy(preds, labels):\n",
        "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
      ],
      "metadata": {
        "id": "plC1FWqiP01v"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import datetime\n",
        "\n",
        "def format_time(elapsed):\n",
        "    '''\n",
        "    Takes a time in seconds and returns a string hh:mm:ss\n",
        "    '''\n",
        "    # Round to the nearest second.\n",
        "    elapsed_rounded = int(round((elapsed)))\n",
        "    \n",
        "    # Format as hh:mm:ss\n",
        "    return str(datetime.timedelta(seconds=elapsed_rounded))"
      ],
      "metadata": {
        "id": "hEAVuckQP3bv"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# tell pytorch to use the gpu if available\n",
        "if torch.cuda.is_available():    \n",
        "      \n",
        "    device = torch.device(\"cuda\")\n",
        "\n",
        "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
        "\n",
        "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
        "\n",
        "# If not...\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "--p6dSOJP52z",
        "outputId": "e39f0e81-f278-4d7f-f2f6-83856b61ff93"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are 1 GPU(s) available.\n",
            "We will use the GPU: Tesla T4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "# This training code is based on the `run_glue.py` script here:\n",
        "# https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L128\n",
        "# Set the seed value all over the place to make this reproducible.\n",
        "seed_val = 42\n",
        "random.seed(seed_val)\n",
        "np.random.seed(seed_val)\n",
        "torch.manual_seed(seed_val)\n",
        "torch.cuda.manual_seed_all(seed_val)\n",
        "loss_values = []\n",
        "for epoch_i in range(0, epochs):\n",
        "    #Training \n",
        "    print(\"\")\n",
        "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
        "    print('Training...')\n",
        "    # Measure how long the training epoch takes.\n",
        "    t0 = time.time()\n",
        "    # Reset the total loss for this epoch.\n",
        "    total_loss = 0\n",
        "    bert_model.train()\n",
        "    # For each batch of training data...\n",
        "    for step, batch in enumerate(bert_train_dataloader):\n",
        "      #Report progress after every 40 epochs\n",
        "        if step % 40 == 0 and not step == 0: \n",
        "            # Calculate elapsed time in minutes.\n",
        "            elapsed = format_time(time.time() - t0)\n",
        "            \n",
        "            # print current training batch and elapsed time\n",
        "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(bert_train_dataloader), elapsed))\n",
        "        \n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_input_mask = batch[1].to(device)\n",
        "        b_labels = batch[2].to(device)\n",
        "        \n",
        "        bert_model.zero_grad()        \n",
        "        \n",
        "        outputs = bert_model(b_input_ids, \n",
        "                    token_type_ids=None, \n",
        "                    attention_mask=b_input_mask, \n",
        "                    labels=b_labels)\n",
        "        \n",
        "        # model returns a tuple, extract loss value from that tuple\n",
        "        loss = outputs[0]\n",
        "        total_loss += loss.item()\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(bert_model.parameters(), 1.0)\n",
        "        bert_optimizer.step()\n",
        "        \n",
        "        bert_scheduler.step()\n",
        "    # Calculate the average loss over the training data.\n",
        "    avg_train_loss = total_loss / len(bert_train_dataloader)            \n",
        "    \n",
        "    # Store the loss value for plotting the learning curve.\n",
        "    loss_values.append(avg_train_loss)\n",
        "    print(\"\")\n",
        "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
        "    print(\"  Training epcoh took: {:}\".format(format_time(time.time() - t0)))\n",
        "        \n",
        "    #Validation Part\n",
        "\n",
        "    \n",
        "    print(\"\")\n",
        "    print(\"Running Validation...\")\n",
        "    t0 = time.time()\n",
        "    # Put the model in evaluation mode    \n",
        "    bert_model.eval()\n",
        "    # Tracking variables \n",
        "    eval_loss, eval_accuracy = 0, 0\n",
        "    nb_eval_steps, nb_eval_examples = 0, 0\n",
        "    # Evaluate data for one epoch\n",
        "    for batch in bert_validation_dataloader:\n",
        "        \n",
        "        # Add batch to GPU\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        \n",
        "        # Unpack the inputs from our dataloader\n",
        "        b_input_ids, b_input_mask, b_labels = batch\n",
        "        \n",
        "        with torch.no_grad():        \n",
        "           outputs = bert_model(b_input_ids, \n",
        "                            token_type_ids=None, \n",
        "                            attention_mask=b_input_mask)\n",
        "        \n",
        "        logits = outputs[0]\n",
        "        # Move logits and labels to CPU\n",
        "        logits = logits.detach().cpu().numpy()\n",
        "        label_ids = b_labels.to('cpu').numpy()\n",
        "        \n",
        "        # Calculate the accuracy for this batch of test sentences.\n",
        "        tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
        "        \n",
        "        # Accumulate the total accuracy.\n",
        "        eval_accuracy += tmp_eval_accuracy\n",
        "        # Track the number of batches\n",
        "        nb_eval_steps += 1\n",
        "    # Report the final accuracy for this validation run.\n",
        "    print(\"  Accuracy: {0:.2f}\".format(eval_accuracy/nb_eval_steps))\n",
        "    print(\"  Validation took: {:}\".format(format_time(time.time() - t0)))\n",
        "print(\"\")\n",
        "print(\"Training complete!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 520
        },
        "id": "Mby_M6bqP8N9",
        "outputId": "dde0e6e3-07fb-469c-cc9f-28333b60b964"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======== Epoch 1 / 2 ========\n",
            "Training...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-f9c604f545c4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     38\u001b[0m                     \u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m                     \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mb_input_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m                     labels=b_labels)\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0;31m# model returns a tuple, extract loss value from that tuple\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1559\u001b[0m             \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1560\u001b[0m             \u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1561\u001b[0;31m             \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1562\u001b[0m         )\n\u001b[1;32m   1563\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1022\u001b[0m             \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m             \u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1024\u001b[0;31m             \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1025\u001b[0m         )\n\u001b[1;32m   1026\u001b[0m         \u001b[0msequence_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    608\u001b[0m                     \u001b[0mencoder_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    609\u001b[0m                     \u001b[0mpast_key_value\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 610\u001b[0;31m                     \u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    611\u001b[0m                 )\n\u001b[1;32m    612\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    492\u001b[0m             \u001b[0mhead_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    493\u001b[0m             \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 494\u001b[0;31m             \u001b[0mpast_key_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself_attn_past_key_value\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    495\u001b[0m         )\n\u001b[1;32m    496\u001b[0m         \u001b[0mattention_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself_attention_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    424\u001b[0m             \u001b[0mencoder_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    425\u001b[0m             \u001b[0mpast_key_value\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 426\u001b[0;31m             \u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    427\u001b[0m         )\n\u001b[1;32m    428\u001b[0m         \u001b[0mattention_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    357\u001b[0m         \u001b[0mcontext_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattention_probs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue_layer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 359\u001b[0;31m         \u001b[0mcontext_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontext_layer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    360\u001b[0m         \u001b[0mnew_context_layer_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontext_layer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall_head_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    361\u001b[0m         \u001b[0mcontext_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontext_layer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_context_layer_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 48.00 MiB (GPU 0; 14.76 GiB total capacity; 13.85 GiB already allocated; 5.75 MiB free; 13.96 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Plotting the training loss over epochs\n",
        "import plotly.express as px\n",
        "f = pd.DataFrame(loss_values)\n",
        "f.columns=['Loss']\n",
        "fig = px.line(f, x=f.index, y=f.Loss)\n",
        "fig.update_layout(title='Training loss of the Model',\n",
        "                   xaxis_title='Epoch',\n",
        "                   yaxis_title='Loss')\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "1c0KpxM0QDcH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bert_prediction_sampler = SequentialSampler(bert_test_dataset)\n",
        "bert_prediction_dataloader = DataLoader(bert_test_dataset, sampler=bert_prediction_sampler, batch_size=batch_size)"
      ],
      "metadata": {
        "id": "fhkym7i5QD-X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prediction on test set\n",
        "\n",
        "print('Predicting labels for {:,} test sentences...'.format(len(bert_input_ids_test)))\n",
        "\n",
        "# Put model in evaluation mode\n",
        "bert_model.eval()\n",
        "\n",
        "# Tracking variables \n",
        "predictions , true_labels = [], []"
      ],
      "metadata": {
        "id": "uAw_IjWQQGX5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict \n",
        "for batch in bert_prediction_dataloader:\n",
        "  # Add batch to GPU\n",
        "  batch = tuple(t.to(device) for t in batch)\n",
        "  \n",
        "  # Unpack the inputs from our dataloader\n",
        "  b_input_ids, b_input_mask, b_labels = batch\n",
        "  \n",
        " \n",
        "  with torch.no_grad():\n",
        "      # Forward pass, calculate logit predictions\n",
        "      outputs = bert_model(b_input_ids, token_type_ids=None, \n",
        "                      attention_mask=b_input_mask)\n",
        "\n",
        "  logits = outputs[0]\n",
        "\n",
        "  # Move logits and labels to CPU\n",
        "  logits = logits.detach().cpu().numpy()\n",
        "  label_ids = b_labels.to('cpu').numpy()\n",
        "  \n",
        "  # Store predictions and true labels\n",
        "  predictions.append(logits)\n",
        "  true_labels.append(label_ids)\n",
        "print('    DONE.')"
      ],
      "metadata": {
        "id": "ZYZ-0WAfS48_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions_labels = [item for subitem in predictions for item in subitem]\n",
        "\n",
        "predictions_labels = np.argmax(predictions_labels, axis=1).flatten()\n",
        "\n",
        "# Combine the correct labels for each batch into a single list.\n",
        "flat_true_labels = [item for sublist in true_labels for item in sublist]\n",
        "\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "print (classification_report(predictions_labels, flat_true_labels))\n",
        "print(confusion_matrix(flat_true_labels, predictions_labels))"
      ],
      "metadata": {
        "id": "WzFkkW11QTIb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "# This training code is based on the `run_glue.py` script here:\n",
        "# https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L128\n",
        "# Set the seed value all over the place to make this reproducible.\n",
        "seed_val = 42\n",
        "random.seed(seed_val)\n",
        "np.random.seed(seed_val)\n",
        "torch.manual_seed(seed_val)\n",
        "torch.cuda.manual_seed_all(seed_val)\n",
        "loss_values = []\n",
        "# For each epoch...\n",
        "for epoch_i in range(0, epochs):\n",
        "    \n",
        "    # Training\n",
        "\n",
        "    print(\"\")\n",
        "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
        "    print('Training...')\n",
        "    # Measure how long the training epoch takes.\n",
        "    t0 = time.time()\n",
        "    # Reset the total loss for this epoch.\n",
        "    total_loss = 0\n",
        "    roberta_model.train()\n",
        "    # For each batch of training data...\n",
        "    for step, batch in enumerate(roberta_train_dataloader):\n",
        "        # Report progress after every 40 epochs\n",
        "        if step % 40 == 0 and not step == 0:\n",
        "            # Calculate elapsed time in minutes.\n",
        "            elapsed = format_time(time.time() - t0)\n",
        "            \n",
        "            #Printing current batch and elapsed time\n",
        "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(roberta_train_dataloader), elapsed))\n",
        "        \n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_input_mask = batch[1].to(device)\n",
        "        b_labels = batch[2].to(device)\n",
        "        \n",
        "        roberta_model.zero_grad()        \n",
        "        \n",
        "        outputs = roberta_model(b_input_ids, \n",
        "                    token_type_ids=None, \n",
        "                    attention_mask=b_input_mask, \n",
        "                    labels=b_labels)\n",
        "        \n",
        "        # Model returns tuple, extract loss value from that tuple\n",
        "        loss = outputs[0]\n",
        "        \n",
        "        total_loss += loss.item()\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(roberta_model.parameters(), 1.0)\n",
        "        roberta_optimizer.step()\n",
        "        \n",
        "        roberta_scheduler.step()\n",
        "    # Calculate the average loss over the training data.\n",
        "    avg_train_loss = total_loss / len(roberta_train_dataloader)            \n",
        "    \n",
        "    # Store the loss value for plotting the learning curve.\n",
        "    loss_values.append(avg_train_loss)\n",
        "    print(\"\")\n",
        "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
        "    print(\"  Training epcoh took: {:}\".format(format_time(time.time() - t0)))\n",
        "        \n",
        "    # Validation\n",
        "    \n",
        "    print(\"\")\n",
        "    print(\"Running Validation...\")\n",
        "    t0 = time.time()\n",
        "    # Put the model in evaluation mode    \n",
        "    roberta_model.eval()\n",
        "     \n",
        "    eval_loss, eval_accuracy = 0, 0\n",
        "    nb_eval_steps, nb_eval_examples = 0, 0\n",
        "    # Evaluate data for one epoch\n",
        "    for batch in roberta_validation_dataloader:\n",
        "        \n",
        "        # Add batch to GPU\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        \n",
        "        # Unpack the inputs from our dataloader\n",
        "        b_input_ids, b_input_mask, b_labels = batch\n",
        "        \n",
        "        with torch.no_grad():        \n",
        "           \n",
        "            outputs = roberta_model(b_input_ids, \n",
        "                            token_type_ids=None, \n",
        "                            attention_mask=b_input_mask)\n",
        "        \n",
        "        \n",
        "        logits = outputs[0]\n",
        "        # Move logits and labels to CPU\n",
        "        logits = logits.detach().cpu().numpy()\n",
        "        label_ids = b_labels.to('cpu').numpy()\n",
        "        \n",
        "        # Calculate the accuracy for this batch of test sentences.\n",
        "        tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
        "        \n",
        "        # Accumulate the total accuracy.\n",
        "        eval_accuracy += tmp_eval_accuracy\n",
        "        # Track the number of batches\n",
        "        nb_eval_steps += 1\n",
        "    # Report the final accuracy for this validation run.\n",
        "    print(\"  Accuracy: {0:.2f}\".format(eval_accuracy/nb_eval_steps))\n",
        "    print(\"  Validation took: {:}\".format(format_time(time.time() - t0)))\n",
        "print(\"\")\n",
        "print(\"Training complete!\")"
      ],
      "metadata": {
        "id": "qf8iAstvTOAV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Testing "
      ],
      "metadata": {
        "id": "INAvtKRCTTVs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "roberta_prediction_sampler = SequentialSampler(roberta_test_dataset)\n",
        "roberta_prediction_dataloader = DataLoader(roberta_test_dataset, sampler=roberta_prediction_sampler, batch_size=batch_size)"
      ],
      "metadata": {
        "id": "slR7-5oZTPp0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prediction on test set\n",
        "\n",
        "print('Predicting labels for {:,} test sentences...'.format(len(bert_input_ids_test)))\n",
        "\n",
        "# Put model in evaluation mode\n",
        "roberta_model.eval()\n",
        "\n",
        "# Tracking variables \n",
        "predictions , true_labels = [], []"
      ],
      "metadata": {
        "id": "pQJ1X-62TWMV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict \n",
        "for batch in roberta_prediction_dataloader:\n",
        "  # Add batch to GPU\n",
        "  batch = tuple(t.to(device) for t in batch)\n",
        "  \n",
        "  # Unpack the inputs from our dataloader\n",
        "  b_input_ids, b_input_mask, b_labels = batch\n",
        "\n",
        "  with torch.no_grad():\n",
        "      \n",
        "      outputs = roberta_model(b_input_ids, token_type_ids=None, \n",
        "                      attention_mask=b_input_mask)\n",
        "\n",
        "  logits = outputs[0]\n",
        "\n",
        "  \n",
        "  logits = logits.detach().cpu().numpy()\n",
        "  label_ids = b_labels.to('cpu').numpy()\n",
        "  \n",
        "  # Store predictions and true labels\n",
        "  predictions.append(logits)\n",
        "  true_labels.append(label_ids)\n",
        "print('    DONE.')"
      ],
      "metadata": {
        "id": "V0fm1EA6TbaS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions_labels = [item for subitem in predictions for item in subitem]\n",
        "\n",
        "predictions_labels = np.argmax(predictions_labels, axis=1).flatten()\n",
        "\n",
        "# Combine the correct labels for each batch into a single list.\n",
        "flat_true_labels = [item for sublist in true_labels for item in sublist]\n",
        "\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "print (classification_report(predictions_labels, flat_true_labels))\n",
        "print(confusion_matrix(flat_true_labels, predictions_labels))"
      ],
      "metadata": {
        "id": "_qQyMPEWTgYJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}